{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này, chúng ta sẽ không giới thiệu bất kỳ kiến thức mới nào. Thay vào đó, chúng ta sẽ tái cấu trúc lại mã code từ chương trước bằng cách sử dụng các class để tạo cấu trúc code dễ đọc hơn và giống với LLAMA2 hơn.\n",
    "\n",
    "Lý do tôi chia chương này thành một phần riêng là:\n",
    "\n",
    "- Thứ nhất, việc đọc chương này có thể giúp chúng ta củng cố kiến thức đã học và tạo sự tự tin hơn.\n",
    "\n",
    "- Thứ hai, đối với những người không quá quen thuộc với việc sử dụng class trong lập trình, việc này có thể giúp họ dễ dàng hơn khi làm quen với khái niệm này.\n",
    "\n",
    "Trong chương này, tôi sẽ không giải thích nhiều vì như tôi đã nói, chúng ta không đưa thêm kiến thức mới nào vào đây. Bạn chỉ cần sao chép mã code ở đây và thử thực hiện, bạn sẽ thấy nó tương tự với các đoạn code ở chương trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "from einops import rearrange # einstein operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 219])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 20\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "subset_dataset = dataset['train'][:sample]['text']\n",
    "tokenized_dataset = tokenizer(\n",
    "    subset_dataset,\n",
    "    return_tensors='pt',\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True  # Enable truncation\n",
    ")\n",
    "\n",
    "data = tokenized_dataset['input_ids']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs:\n",
    "    def __init__(self, sequence_len, vocab_size):\n",
    "        self.batch_size = 16\n",
    "        self.n_head = 4\n",
    "        self.n_embd = 36\n",
    "        self.sequence_len = sequence_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "args = ModelArgs(sequence_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 218]), torch.Size([16, 218]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size):\n",
    "    idx = torch.randint(0, len(data), size=(batch_size,))\n",
    "    batch = data[idx]\n",
    "\n",
    "    xb = batch[:, :-1].contiguous()\n",
    "    yb = batch[:, 1:].contiguous()\n",
    "    \n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(data, args.batch_size)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "<img src=\"images/chap1/embedding.png\" alt=\"Embedding Architecture\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.sequence_len = args.sequence_len\n",
    "        \n",
    "        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n",
    "        self.position = nn.Embedding(args.sequence_len, args.n_embd)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        token_embd = self.wte(input_ids)\n",
    "        position_embd = self.position(torch.arange(self.sequence_len))\n",
    "        \n",
    "        input_ids_embd = token_embd + position_embd        \n",
    "        \n",
    "        return input_ids_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd = Embedding(args)\n",
    "x_embd = embd(xb)\n",
    "x_embd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "<img src=\"images/chap1/self_attention.png\" alt=\"Self Attention\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.head_dim = args.n_embd // args.n_head\n",
    "        opt_size = args.n_head * self.head_dim\n",
    "        hidden_size = args.n_embd\n",
    "        \n",
    "        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n",
    "        self.out_proj = nn.Linear(opt_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_ids_embd_norm):\n",
    "        seq_len = input_ids_embd_norm.shape[1]\n",
    "        \n",
    "        qkv = self.Wqkv(input_ids_embd_norm)\n",
    "        qkv = rearrange(qkv, 'b t (three h d) -> b t three h d', three=3, d=self.head_dim)\n",
    "        q, k, v = qkv.unbind(2)\n",
    "        \n",
    "        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum(\"bthd, bshd -> bhts\", q, k * softmax_scale)\n",
    "        \n",
    "        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n",
    "        scores += mask\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.einsum(\"bhts, bshd -> bthd\", attention_weights, v)\n",
    "        output = rearrange(output, \"... h d -> ... (h d)\")\n",
    "\n",
    "        attn_out = self.out_proj(output)\n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "attn_norm = nn.LayerNorm(args.n_embd)\n",
    "x_embd_norm = attn_norm(x_embd)\n",
    "\n",
    "attn = Attention(args)\n",
    "attn_out = attn(x_embd_norm)\n",
    "# add residual\n",
    "attn_out += x_embd\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "<img src=\"images/chap1/feed_forward.png\" alt=\"Feed Forward\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        hidden_size = 4 * args.n_embd\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.n_embd, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, args.n_embd)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, attn_out_norm):\n",
    "        hidden_states = self.fc1(attn_out_norm)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        ffwd_out = self.fc2(hidden_states)\n",
    "        \n",
    "        return ffwd_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "ffwd_norm = nn.LayerNorm(args.n_embd)\n",
    "attn_out_norm = ffwd_norm(attn_out)\n",
    "\n",
    "ffwd = FeedForward(args)\n",
    "ffwd_out = ffwd(attn_out_norm)\n",
    "# add residual\n",
    "ffwd_out += attn_out\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer Block\n",
    "\n",
    "<img src=\"images/chap1/transformer_block.png\" alt=\"Transformer Block\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransfomerBlock(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_norm = nn.LayerNorm(args.n_embd)\n",
    "        self.ffwd_norm = nn.LayerNorm(args.n_embd)\n",
    "        \n",
    "        self.attn = Attention(args)\n",
    "        self.ffwd = FeedForward(args)\n",
    "        \n",
    "    def forward(self, input_ids_embd):\n",
    "        \n",
    "        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n",
    "        \n",
    "        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n",
    "        \n",
    "        return ffwd_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_block = TransfomerBlock(args)\n",
    "ffwd_out = t_block(x_embd)\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "<img src=\"images/chap1/output_prob.png\" alt=\"Output Probabilities\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 50257])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(args.n_embd)\n",
    "        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n",
    "        \n",
    "    def forward(self, ffwd_out):\n",
    "        ffwd_out_norm = self.norm(ffwd_out)\n",
    "        logits = self.linear(ffwd_out_norm)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "t_head = TransformerHead(args)\n",
    "logits = t_head(ffwd_out)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSequential(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_layer = 2\n",
    "        \n",
    "        modules = [Embedding(args)]\n",
    "        modules += [TransfomerBlock(args) for _ in range(n_layer)]\n",
    "        modules.append(TransformerHead(args))\n",
    "        \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        logits = self.layers(input_ids)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 50257])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerSequential(args)\n",
    "logits = model(xb)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        labels = labels.view(-1)                    \n",
    "                             \n",
    "        loss = self.loss_fct(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0337, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_loss = TransformerLoss()\n",
    "loss = t_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chap1/attention.png\" alt=\"Transformers\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.1727, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenized_dataset['input_ids']\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "args = ModelArgs(sequence_len, vocab_size)\n",
    "xb, yb = get_batch(data, args.batch_size)\n",
    "\n",
    "model = TransformerSequential(args)\n",
    "logits = model(xb)\n",
    "\n",
    "t_loss = TransformerLoss()\n",
    "loss = t_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta đã hoàn thành được cái cơ bản, 'backbone' của kiến trúc transformer trong LLAMA2 rồi. Bây giờ hãy chuyển sang chương tiếp theo và khám phá sâu hơn về kiến trúc thực sự của LLAMA2 nhé."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
