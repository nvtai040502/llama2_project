{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong chương này, mục tiêu hàng đầu của chúng ta là khám phá một cách chi tiết và cụ thể từng bước của quá trình giải mã (decoder) (phần được khoanh đỏ) dựa trên kiến trúc Attention, như hình minh họa dưới đây:\n",
    "\n",
    "<img src=\"images/attention.png\" alt=\"Attention Architecture\" width=\"300\"/>\n",
    "\n",
    "Chúng ta sẽ đảm bảo rằng mỗi bước trong quy trình này được diễn giải một cách chi tiết để chúng ta có thể hiểu sâu hơn về cách nó hoạt động và tương tác với dữ liệu đầu vào. Điều quan trọng là thông qua việc làm này, chúng ta sẽ có cơ hội thấu hiểu rõ hơn về cách áp dụng kiến thức này vào các dự án thực tế, ví dụ như xây dựng một Mô hình Ngôn ngữ Lớn (Large Language Model) cho tiếng Việt, mở ra nhiều tiềm năng ứng dụng hấp dẫn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "with open('data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(type(text))    \n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tập dữ liệu \"tiny Shakespeare\" là một kho văn bản chứa các tác phẩm của danh tác William Shakespeare, với hơn 1 triệu ký tự. Mục tiêu chính của việc sử dụng tập dữ liệu này là xây dựng một mô hình mạng neural có khả năng dự đoán ký tự tiếp theo trong một đoạn văn dựa trên các ký tự trước đó. Mô hình này sẽ có khả năng tái tạo cấu trúc và phong cách viết của Shakespeare, tạo ra văn bản một cách tự nhiên và đầy hấp dẫn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 200 characters\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "chars = sorted(set(text))\n",
    "print(''.join(chars))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong dự án này, chúng ta sẽ sử dụng một phương pháp nhúng (embedding) đơn giản. Cụ thể, chúng ta sẽ xác định tất cả các ký tự duy nhất có trong toàn bộ tập dữ liệu và gán một số duy nhất cho mỗi ký tự này.\n",
    "\n",
    "Trong tập dữ liệu của chúng ta, có tổng cộng 65 ký tự khác nhau. Mục tiêu chính của dự án là xây dựng một mô hình có khả năng dự đoán ký tự tiếp theo nằm trong 65 ký tự này. Điều này có nghĩa rằng chúng ta muốn mô hình học cách dự đoán ký tự tiếp theo dựa trên ngữ cảnh và phân tích các ký tự trước đó trong chuỗi văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Encoding\n",
    "stoi = {s:i for i, s in enumerate(chars)}\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l:  ''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây, chúng ta đang thực hiện một quá trình được gọi là \"mã hóa ký tự (character encoding)\". Trong quá trình này, mỗi ký tự riêng biệt trong dữ liệu của chúng ta sẽ được ánh xạ thành một số nguyên tương ứng. Chúng ta thực hiện việc này để có khả năng chuyển đổi linh hoạt giữa chuỗi ký tự và số nguyên, giúp chúng ta hiểu và biểu diễn kết quả một cách dễ dàng và hiệu quả hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "text_exp = \"Hello World\"\n",
    "print(encode(text_exp))\n",
    "print(decode(encode(text_exp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all text data to integers\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "data[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuyển mọi ký tự trong dữ liệu \"Tiny Shakespeare\" sang số nguyên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape: torch.Size([4, 8])\n",
      "yb.shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Define block size and batch size\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "# Generate random indices within the valid range\n",
    "ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n",
    "\n",
    "# Extract blocks of data using the generated indices\n",
    "xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "# Extract corresponding target blocks\n",
    "# Note that yb is reshaped to a 1D tensor\n",
    "yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n",
    "\n",
    "# Print the shapes of xb and yb\n",
    "print(\"xb.shape:\", xb.shape)\n",
    "print(\"yb.shape:\", yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bên trên là một ví dụ minh họa về cách tạo x_batch và y_batch sử dụng batch_size, trong đó x_batch được cố định theo block size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [58] --> Target: 46\n",
      "Input: [58, 46] --> Target: 39\n",
      "Input: [58, 46, 39] --> Target: 58\n",
      "Input: [58, 46, 39, 58] --> Target: 1\n",
      "Input: [58, 46, 39, 58, 1] --> Target: 58\n",
      "Input: [58, 46, 39, 58, 1, 58] --> Target: 46\n",
      "Input: [58, 46, 39, 58, 1, 58, 46] --> Target: 43\n",
      "Input: [58, 46, 39, 58, 1, 58, 46, 43] --> Target: 56\n"
     ]
    }
   ],
   "source": [
    "for i in range(block_size):\n",
    "    inp = xb[0, :i+1].tolist()\n",
    "    target = yb[i]\n",
    "    print(f\"Input: {inp} --> Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "n_epochs = 1500\n",
    "\n",
    "vocab_size = len(chars)\n",
    "n_emb = 32\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 8\n",
    "\n",
    "head_size = 20\n",
    "n_head = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tránh sự lặp lại không cần thiết trong quá trình giải thích cách thực hiện, chúng ta sẽ duy trì liên tục các biến sau đây."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 4225\n"
     ]
    }
   ],
   "source": [
    "# Define the embedding size\n",
    "C = torch.randn(vocab_size, n_emb)\n",
    "\n",
    "weight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\n",
    "bias = torch.zeros(vocab_size)\n",
    "\n",
    "parameters = [C, weight, bias]\n",
    "\n",
    "num_parameters = 0\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_emb.shape: torch.Size([4, 8, 32])\n",
      "logits.shape: torch.Size([4, 8, 65])\n",
      "loss: tensor(4.7028, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward sample\n",
    "x_emb = C[xb]  # Embedding lookup for input data\n",
    "print(\"x_emb.shape:\", x_emb.shape)\n",
    "\n",
    "# Compute logits using a linear transformation\n",
    "logits = x_emb @ weight + bias\n",
    "print(\"logits.shape:\", logits.shape)\n",
    "\n",
    "# Reshape logits for the cross-entropy loss\n",
    "logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "# Compute the cross-entropy loss\n",
    "loss = F.cross_entropy(logits, yb)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4565, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(parameters, lr = learning_rate)\n",
    "\n",
    "for epochi in range(n_epochs):\n",
    "    # Generate random indices within the valid range\n",
    "    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n",
    "\n",
    "    # Extract blocks of data using the generated indices\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # Extract corresponding target blocks\n",
    "    # Note that yb is reshaped to a 1D tensor\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n",
    "\n",
    "    x_emb = C[xb]  # Embedding lookup for input data\n",
    "\n",
    "    # Compute logits using a linear transformation\n",
    "    logits = x_emb @ weight + bias\n",
    "\n",
    "    # Reshape logits for the cross-entropy loss\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 4481\n"
     ]
    }
   ],
   "source": [
    "C = torch.randn(vocab_size, n_emb)\n",
    "\n",
    "weight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\n",
    "bias = torch.zeros(vocab_size)\n",
    "\n",
    "# New code\n",
    "# ------------------------------------------------------------\n",
    "position = torch.randn(block_size, n_emb) * block_size **-0.5\n",
    "parameters = [C, weight, bias, position]\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "num_parameters = 0\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phương pháp \"Distributed Presentation,\" ban đầu chúng ta đã sử dụng việc nhúng (embedding) để ánh xạ từng từ vào một vector đặc trưng riêng biệt (matrix C). Tuy nhiên, để nâng cao khả năng biểu diễn, chúng ta muốn không chỉ biết về vector đặc trưng (C) của ký tự mà còn quan tâm đến vị trí (position) của ký tự đó trong câu. \n",
    "\n",
    "Để thực hiện điều này, chúng ta sẽ tạo ra một ma trận vị trí mới (matrix position). Trong ma trận này, mỗi hàng sẽ tương ứng với một vị trí trong câu và nó sẽ được sử dụng để kết hợp với vector đặc trưng (C) của ký tự tại vị trí tương ứng, giúp cải thiện khả năng biểu diễn của mô hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 8]), torch.Size([512]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    # Generate random indices within the valid range\n",
    "    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n",
    "\n",
    "    # Extract blocks of data using the generated indices\n",
    "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
    "\n",
    "    # Extract corresponding target blocks\n",
    "    # Note that yb is reshaped to a 1D tensor\n",
    "    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n",
    "    \n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(data, batch_size, block_size)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tránh việc lặp lại mã code và để tạo sự tiện lợi, tôi tạo một hàm có tên là get_batch để tự động tạo các batch x và y dựa trên kích thước batch_size và block_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4990, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(parameters, lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # New code\n",
    "    # -----------------------------------------------\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "    \n",
    "    # Embedding lookup for input data\n",
    "    x_emb = C[xb]  \n",
    "    x_emb += position\n",
    "    # -----------------------------------------------\n",
    "    \n",
    "    # Compute logits using a linear transformation\n",
    "    logits = x_emb @ weight + bias\n",
    "\n",
    "    # Reshape logits for the cross-entropy loss\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    loss = F.cross_entropy(logits, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weight Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiện tại, mô hình chỉ dựa vào ký tự và vị trí gần nhất của ký tự đó để thực hiện dự đoán. Tuy nhiên, điều này không đủ hiệu quả. Chúng ta muốn mô hình có khả năng sử dụng tất cả thông tin từ các ký tự trước đó để cải thiện dự đoán ký tự tiếp theo.\n",
    "\n",
    "Hãy xem xét ví dụ từ hai từ \"his\" và \"like.\" Giả sử chúng ta cung cấp cho mô hình các vector đặc trưng biểu diễn cho từ \"i\" và vị trí thứ hai của từ \"i\" trong câu. Tuy nhiên, trong trường hợp này, mô hình có thể dự đoán ký tự tiếp theo là \"s\" hoặc \"k\" mà không có thông tin đủ để quyết định. Điều quan trọng là chúng ta cần cung cấp cho mô hình thông tin về ký tự \"h\" đứng trước ký tự \"i\" thay vì \"l\" để mô hình có thể học được và dự đoán đúng ký tự \"s\" là ký tự tiếp theo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# Lower triangular matrix for masking\n",
    "tril = torch.tril(torch.ones(block_size, block_size))\n",
    "\n",
    "# Masking to make sure the network can't attend to the future positions\n",
    "wei = tril.masked_fill(tril==0, float('-inf'))\n",
    "\n",
    "# Applying softmax to get the attention probabilities\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do đó, chiến lược tạm thời của chúng ta ở đây là tích hợp thông tin từ tất cả các ký tự trước đó, đã xuất hiện, bằng cách tính trung bình của các vector biểu diễn và vị trí của chúng. Điều này sẽ giúp mô hình dự đoán từ tiếp theo một cách chính xác hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Input (first batch, first 3 characters):\n",
      " tensor([[-0.4335,  0.2487, -1.6515,  2.6138,  0.4584,  1.0921, -1.7675, -0.7268,\n",
      "          2.0422,  2.2848,  0.9093,  0.6099,  1.3877, -0.3385, -3.3128, -0.5631,\n",
      "         -0.7516,  0.2281, -0.5588,  1.6799, -0.8824,  0.6582,  0.7420, -0.1393,\n",
      "         -1.3523, -0.5295,  0.8053, -1.7366, -0.1735, -0.6012, -1.0308, -1.0555],\n",
      "        [-1.0184, -2.0927, -0.3102,  3.6519, -2.4072,  0.0139,  0.9342, -1.7704,\n",
      "         -0.1125, -0.5072, -1.2626, -1.7496, -1.1825,  0.0487, -0.9318,  0.1392,\n",
      "         -2.3752,  1.4060, -1.2250,  1.9381,  0.3784, -1.2098,  0.6793,  0.8746,\n",
      "         -0.5673, -3.0030,  1.0940, -1.0829,  0.0083,  2.4880, -0.3996,  2.7292],\n",
      "        [ 0.4108,  0.8699, -1.0485, -3.0167,  0.0901, -0.1466, -0.6756,  0.8492,\n",
      "          0.2072,  1.3359, -0.2287,  1.1866, -1.0809, -1.1253, -0.8090, -1.1819,\n",
      "         -2.4869, -3.1272,  0.2352,  0.4746,  0.2054, -1.5705, -1.3706, -0.6790,\n",
      "          0.5017, -2.9568,  0.0115, -0.3515,  1.4428, -1.3596, -0.5216,  0.8348]])\n",
      "\n",
      "Transformed Input (first batch, first 3 characters):\n",
      " tensor([[-0.4335,  0.2487, -1.6515,  2.6138,  0.4584,  1.0921, -1.7675, -0.7268,\n",
      "          2.0422,  2.2848,  0.9093,  0.6099,  1.3877, -0.3385, -3.3128, -0.5631,\n",
      "         -0.7516,  0.2281, -0.5588,  1.6799, -0.8824,  0.6582,  0.7420, -0.1393,\n",
      "         -1.3523, -0.5295,  0.8053, -1.7366, -0.1735, -0.6012, -1.0308, -1.0555],\n",
      "        [-0.7260, -0.9220, -0.9808,  3.1329, -0.9744,  0.5530, -0.4166, -1.2486,\n",
      "          0.9648,  0.8888, -0.1766, -0.5698,  0.1026, -0.1449, -2.1223, -0.2119,\n",
      "         -1.5634,  0.8171, -0.8919,  1.8090, -0.2520, -0.2758,  0.7107,  0.3676,\n",
      "         -0.9598, -1.7663,  0.9496, -1.4098, -0.0826,  0.9434, -0.7152,  0.8369],\n",
      "        [-0.3471, -0.3247, -1.0034,  1.0830, -0.6196,  0.3198, -0.5029, -0.5493,\n",
      "          0.7123,  1.0379, -0.1940,  0.0157, -0.2919, -0.4717, -1.6845, -0.5353,\n",
      "         -1.8712, -0.4977, -0.5162,  1.3642, -0.0995, -0.7074,  0.0169,  0.0187,\n",
      "         -0.4726, -2.1631,  0.6369, -1.0570,  0.4259,  0.1757, -0.6507,  0.8362]])\n"
     ]
    }
   ],
   "source": [
    "C = torch.randn(vocab_size, n_emb)\n",
    "position = torch.randn(block_size, n_emb)\n",
    "\n",
    "x_emb = C[xb]\n",
    "x_emb +=  position\n",
    "\n",
    "# New code\n",
    "# -------------------------------------------------------\n",
    "# Lower triangular matrix for masking\n",
    "tril = torch.tril(torch.ones(block_size, block_size))\n",
    "\n",
    "# Masking to make sure the network can't attend to the future positions\n",
    "wei = tril.masked_fill(tril==0, float('-inf'))\n",
    "\n",
    "# Applying softmax to get the attention probabilities\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ x_emb\n",
    "# --------------------------------------------------------\n",
    "\n",
    "# Print the first 3 elements of the original input and the transformed input for the first block\n",
    "print(f\"Original Input (first batch, first 3 characters):\\n {x_emb[0, :3]}\")\n",
    "print(\"\")\n",
    "print(f\"Transformed Input (first batch, first 3 characters):\\n {out[0, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 4481\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding layers\n",
    "        self.C = torch.randn(vocab_size, n_emb) * vocab_size ** -0.5\n",
    "        self.position = torch.randn(block_size, n_emb) * block_size **-0.5\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        self.weight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\n",
    "        self.bias = torch.zeros(vocab_size)\n",
    "        \n",
    "        self.parameters = [self.C, self.weight, self.bias, self.position]\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    def forward(self, inp, targets):\n",
    "        # Embedding lookup for input data\n",
    "        x_emb = self.C[inp] \n",
    "        x_emb += position\n",
    "        \n",
    "        # New code\n",
    "        # -------------------------------------------------------------\n",
    "        # Lower triangular matrix for masking\n",
    "        tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        \n",
    "        # Masking to make sure the network can't attend to the future positions\n",
    "        wei = tril.masked_fill(tril==0, float('-inf'))\n",
    "        \n",
    "        # Applying softmax to get the attention probabilities\n",
    "        wei = F.softmax(wei, dim=1)\n",
    "        \n",
    "        out = wei @ x_emb\n",
    "        logits = out @ weight + bias\n",
    "        # -------------------------------------------------------------\n",
    "        \n",
    "        # Reshape logits for the cross-entropy loss\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters:\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0674, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters, lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Key, Query, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình hiện tại của chúng ta vẫn chưa đủ hiệu quả, vì chúng ta cần xem xét xác suất quan trọng của các từ đã xuất hiện trước đó đối với việc dự đoán ký tự tiếp theo. Hãy xem xét ví dụ với các từ thay vì ký tự, vì tôi nghĩ điều này có thể giúp chúng ta hiểu rõ hơn.\n",
    "\n",
    "Ví dụ, trong câu \"He is a boy,\" để dự đoán từ \"boy,\" các từ \"he\" và \"is\" sẽ có đóng góp quan trọng hơn so với từ \"a\" trong quá trình dự đoán. Điều này có nghĩa là mô hình cần hiểu được sự liên kết ngữ cảnh giữa các từ và xác định xem từ nào có ảnh hưởng lớn đến dự đoán của mình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 8])\n",
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.0000e+00, 4.5888e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.2766e-02, 9.3893e-01, 3.8305e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [4.0015e-03, 8.9893e-01, 3.7121e-05, 9.7027e-02, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.4366e-02, 1.1882e-03, 6.8691e-01, 1.9523e-02, 2.7801e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [1.7339e-15, 1.4864e-16, 9.9915e-01, 8.5241e-04, 5.7747e-15, 1.8133e-14,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.1721e-01, 1.3526e-06, 3.7937e-09, 2.6322e-06, 1.0903e-10, 1.1801e-01,\n",
      "         5.6478e-01, 0.0000e+00],\n",
      "        [6.3539e-10, 5.0136e-11, 9.9992e-01, 7.7084e-05, 2.7238e-08, 1.9036e-14,\n",
      "         2.8848e-12, 4.0533e-11]])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of data\n",
    "xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "C = torch.randn(vocab_size, n_emb)\n",
    "position = torch.randn(block_size, n_emb)\n",
    "# Embed input data\n",
    "x_emb = C[xb]\n",
    "x_emb += position\n",
    "\n",
    "# New code\n",
    "# ------------------------------------------------------\n",
    "# Initialize key and query matrices\n",
    "key = torch.randn(n_emb, head_size) * n_emb ** -0.5\n",
    "query = torch.randn(n_emb, head_size) * n_emb ** -0.5\n",
    "\n",
    "# Calculate the key and query values\n",
    "k = x_emb @ key\n",
    "q = x_emb @ query\n",
    "\n",
    "# Compute the dot product between queries and keys\n",
    "wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "print(wei.shape)\n",
    "\n",
    "tril = torch.tril(torch.ones(block_size, block_size))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) \n",
    "# ------------------------------------------------------\n",
    "\n",
    "wei=F.softmax(wei, dim=-1)\n",
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 20])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New code\n",
    "# ------------------------------------------------------\n",
    "value = torch.randn(n_emb, head_size) * n_emb ** -0.5\n",
    "v = x_emb @ value\n",
    "\n",
    "out = wei @ v\n",
    "# ------------------------------------------------------\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy tưởng tượng rằng bạn là một nhà báo nổi tiếng đang thực hiện một cuộc phỏng vấn với một ngôi sao nổi tiếng, và bạn muốn thu thập thông tin quan trọng từ cuộc trò chuyện đó.\n",
    "\n",
    "- Key có thể coi như danh sách câu hỏi bạn chuẩn bị trước cuộc phỏng vấn. Mỗi câu hỏi là một Key, và mỗi câu hỏi sẽ tập trung vào một khía cạnh cụ thể của cuộc trò chuyện. Ví dụ, một Key có thể là \"Bạn đã từng giành giải Oscar chưa?\"\n",
    "\n",
    "- Value là câu trả lời mà ngôi sao đưa ra cho từng câu hỏi. Mỗi câu trả lời chứa thông tin quan trọng về cuộc trò chuyện, và nó sẽ được lưu trữ và sử dụng sau này khi bạn cần nắm bắt thông tin cụ thể từ cuộc phỏng vấn. Chúng ta có thể coi câu trả lời này là \"value\" của câu hỏi.\n",
    "\n",
    "- Query là cách bạn đặt câu hỏi hoặc tìm kiếm thông tin trong cuộc phỏng vấn. Khi bạn muốn biết điều gì đó cụ thể hoặc muốn nắm bắt một thông tin quan trọng từ cuộc trò chuyện, bạn sẽ đặt câu hỏi hoặc tạo một \"Query\" riêng. Ví dụ, \"Giới thiệu về những vai diễn nổi bật nhất của bạn?\" có thể là một Query.\n",
    "\n",
    "Khi bạn đặt một câu hỏi (Query), mô hình sẽ so sánh nó với danh sách các câu hỏi trước đó (Key) và quyết định câu trả lời nào (Value) chứa thông tin phù hợp nhất với câu hỏi của bạn. Điều này giống như việc bạn tập trung vào câu hỏi cụ thể nào trong cuộc trò chuyện để thu thập thông tin bạn cần."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.weight1 = torch.randn(n_emb, 3 * n_emb) * n_emb ** -0.5\n",
    "        self.bias1 = torch.zeros(3 * n_emb)\n",
    "        \n",
    "        self.weight2 = torch.randn(3 * n_emb, n_emb) * ((3 * n_emb) ** -0.5)\n",
    "        self.bias2 = torch.zeros(n_emb)\n",
    "\n",
    "        self.parameters = [self.weight1, self.bias1, self.weight2, self.bias2]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x @ self.weight1 + self.bias1\n",
    "        x = F.relu(x)\n",
    "        out = x @ self.weight2 + self.bias2\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 11168\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embedding layers\n",
    "        self.C = torch.randn(vocab_size, n_emb) * vocab_size ** -0.5\n",
    "        self.position = torch.randn(block_size, n_emb) * block_size **-0.5\n",
    "        \n",
    "        # Feed-Forward Layer\n",
    "        self.ffwd = FeedFoward(n_emb)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "        # New code\n",
    "        # ---------------------------------------------------------------\n",
    "        self.proj = torch.randn(head_size, n_emb) * head_size ** -0.5\n",
    "        \n",
    "        self.key = torch.randn(n_emb, head_size) * n_emb ** -0.5\n",
    "        self.query = torch.randn(n_emb, head_size) * n_emb ** -0.5\n",
    "        self.value = torch.randn(n_emb, head_size) * n_emb ** -0.5\n",
    "\n",
    "        self.parameters = [self.C, self.position, self.key, self.query, \\\n",
    "                           self.value, self.proj] + self.ffwd.parameters\n",
    "        # ---------------------------------------------------------------\n",
    "\n",
    "        for p in self.parameters:\n",
    "            p.requires_grad = True\n",
    "    \n",
    "    def forward(self, inp, targets):\n",
    "        x_emb = self.C[inp]  # Embedding lookup for input data\n",
    "        x_emb += position\n",
    "\n",
    "        # New code\n",
    "        # -----------------------------------------------------\n",
    "        k = x_emb @ self.key\n",
    "        q = x_emb @ self.query\n",
    "\n",
    "        # Compute the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        \n",
    "        tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = x_emb @ self.value\n",
    "        out = wei @ v\n",
    "        \n",
    "        out = out @ self.proj\n",
    "        \n",
    "        # Feed-Forward\n",
    "        out = self.ffwd(out)\n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "        \n",
    "        # Reshape logits for the cross-entropy loss\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters:\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9821, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters, lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Layer Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Linear(n_emb, 3 * n_emb)\n",
    "        self.fc1 = nn.Linear(3 * n_emb, n_emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inp(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.fc1(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear transformations for key, query, and value\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear transformations for key and query\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei.masked_fill_(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # Linear transformation for value and computing the output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 13537\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.C = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position = nn.Embedding(block_size, n_emb)\n",
    "        \n",
    "        # Single-Head Attention Layer\n",
    "        self.head = Head(head_size)\n",
    "        self.proj = nn.Linear(head_size, n_emb)\n",
    "        \n",
    "        # Feed-Forward Layer\n",
    "        self.ffwd = FeedFoward(n_emb)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "        # New code\n",
    "        # ------------------------------------\n",
    "        # Layer Normalization Layers\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "        self.ln3 = nn.LayerNorm(n_emb)\n",
    "        # ------------------------------------\n",
    "        \n",
    "    def forward(self, inp, targets):\n",
    "        # Embedding lookup for input data\n",
    "        token_emb = self.C(inp)\n",
    "        position_emb = self.position(torch.arange(inp.size(1)))\n",
    "        x_emb = token_emb + position_emb\n",
    "\n",
    "        # New code\n",
    "        # -----------------------------------------------------\n",
    "        # Single-Head Attention\n",
    "        out = self.head(self.ln1(x_emb))\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        out = self.ffwd(self.ln2(out))\n",
    "\n",
    "        # Final layer normalization\n",
    "        out = self.ln3(out)\n",
    "        # ----------------------------------------------------\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "        \n",
    "        # Reshape logits for the cross-entropy loss\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters():\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2237, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 20])\n",
      "torch.Size([64, 8, 20])\n",
      "torch.Size([64, 8, 8])\n",
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [2.7624e-02, 9.7238e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [6.3034e-07, 9.9980e-01, 2.0172e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.8760e-07, 9.9450e-01, 5.5003e-03, 3.7328e-10, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [6.4108e-01, 2.2718e-05, 4.7551e-02, 3.0472e-01, 6.6321e-03, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.5899e-09, 2.0306e-06, 1.2872e-01, 1.9127e-11, 9.4477e-13, 8.7128e-01,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [3.4409e-09, 2.2824e-02, 8.0748e-01, 4.0223e-02, 5.4004e-03, 1.2398e-01,\n",
      "         9.4144e-05, 0.0000e+00],\n",
      "        [1.4672e-07, 9.9970e-01, 3.0362e-04, 2.6681e-13, 2.8710e-12, 1.0302e-06,\n",
      "         2.3637e-08, 2.4413e-09]])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of data\n",
    "xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "C = torch.randn(vocab_size, n_emb)\n",
    "position = torch.randn(block_size, n_emb)\n",
    "# Embed input data\n",
    "x_emb = C[xb]\n",
    "x_emb += position\n",
    "\n",
    "# New code\n",
    "# ----------------------------------------------------------------------\n",
    "# Initialize key and query matrices\n",
    "key_list = [torch.randn(n_emb, head_size // n_head) * \\\n",
    "            n_emb ** -0.5 for headi in range(n_head)]\n",
    "query_list = [torch.randn(n_emb, head_size // n_head) * \\\n",
    "              n_emb ** -0.5 for headi in range(n_head)]\n",
    "\n",
    "# Calculate the key and query values\n",
    "k = torch.stack([x_emb @ key for key in key_list], dim = -1).view\\\n",
    "                                    (xb.shape[0], block_size, -1)\n",
    "q = torch.stack([x_emb @ query for query in query_list], dim = -1).view\\\n",
    "                                    (xb.shape[0], block_size, -1)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(k.shape)\n",
    "print(q.shape)\n",
    "\n",
    "# Compute the dot product between queries and keys\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "print(wei.shape)\n",
    "\n",
    "tril = torch.tril(torch.ones(block_size, block_size))\n",
    "wei = wei.masked_fill(tril==0, float('-inf')) \n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "print(wei[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 8, 20])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New code\n",
    "# ----------------------------------------------------------------------\n",
    "value_list = [torch.randn(n_emb, head_size // n_head) * \\\n",
    "              n_emb ** -0.5 for headi in range(n_head)]\n",
    "\n",
    "v = torch.stack([x_emb @ value for value in value_list], dim = -1).view\\\n",
    "                                        (xb.shape[0], block_size, -1)\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(v.shape)\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_emb, 3 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear transformations for key, query, and value\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear transformations for key and query\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei.masked_fill_(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # Linear transformation for value and computing the output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New code\n",
    "# -----------------------------------------------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size // n_head\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(self.head_size) \\\n",
    "                                    for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply all attention heads in parallel\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        \n",
    "        # Project the concatenated results\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out\n",
    "# -----------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 13537\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.C = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position = nn.Embedding(block_size, n_emb)\n",
    "        \n",
    "        # New code\n",
    "        # -----------------------------------------------------\n",
    "        # Multi-Head Attention Layer\n",
    "        self.mul_head = MultiHeadAttention(n_head, head_size)\n",
    "        # -----------------------------------------------------\n",
    "        \n",
    "        # Feed-Forward Layer\n",
    "        self.ffwd = FeedFoward(n_emb)\n",
    "        \n",
    "        # Layer Normalization Layers\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_emb)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, inp, targets):\n",
    "        # Embedding lookup for input data\n",
    "        token_emb = self.C(inp)\n",
    "        position_emb = self.position(torch.arange(inp.size(1)))\n",
    "        x_emb = token_emb + position_emb\n",
    "        \n",
    "        # New code\n",
    "        # -----------------------------------------------------\n",
    "        # Multi-Head Attention\n",
    "        out = self.mul_head(self.ln1(x_emb))\n",
    "        # ----------------------------------------------------\n",
    "        \n",
    "        # Feed-Forward\n",
    "        out = self.ffwd(self.ln2(out))\n",
    "        \n",
    "        # Final layer normalization\n",
    "        out = self.ln_f(out)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "        \n",
    "        # Reshape logits for the cross-entropy loss\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters():\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2155, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_emb, 3 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear transformations for key, query, and value\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear transformations for key and query\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        wei.masked_fill_(self.tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # Linear transformation for value and computing the output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size // n_head\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(self.head_size) \\\n",
    "                                    for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply all attention heads in parallel\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        \n",
    "        # Project the concatenated results\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 13537\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.C = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position = nn.Embedding(block_size, n_emb)\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.mul_head = MultiHeadAttention(n_head, head_size)\n",
    "        \n",
    "        # Feed-Forward Layer\n",
    "        self.ffwd = FeedFoward(n_emb)\n",
    "        \n",
    "        # Layer Normalization Layers\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_emb)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, inp, targets):\n",
    "        # Embedding lookup for input data\n",
    "        token_emb = self.C(inp)\n",
    "        position_emb = self.position(torch.arange(inp.size(1)))\n",
    "        x_emb = token_emb + position_emb\n",
    "        \n",
    "        # New code\n",
    "        # ---------------------------------------------------------\n",
    "        # Multi-Head Attention\n",
    "        out = x_emb + self.mul_head(self.ln1(x_emb))\n",
    "        \n",
    "        # Feed-Forward\n",
    "        out = out + self.ffwd(self.ln2(out))\n",
    "        # ---------------------------------------------------------\n",
    "        \n",
    "        # Final layer normalization\n",
    "        out = self.ln_f(out)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "        \n",
    "        # Reshape logits for the cross-entropy loss\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters():\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2127, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_emb, 3 * n_emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3 * n_emb, n_emb)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_emb, block_size, head_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear transformations for key, query, and value\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        \n",
    "        # Lower triangular matrix for masking\n",
    "        self.tril = torch.tril(torch.ones(block_size, block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Linear transformations for key and query\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        \n",
    "        # Compute the attention weights\n",
    "        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        \n",
    "        # Masking to make sure the network can't attend to the future positions\n",
    "        wei.masked_fill_(self.tril == 0, float('-inf'))\n",
    "        \n",
    "        # Applying softmax to get the attention probabilities\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        # Linear transformation for value and computing the output\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_emb, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.heads = nn.ModuleList([Head(n_emb, block_size, head_size) for _ in range(n_head)])\n",
    "        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply all attention heads in parallel\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        \n",
    "        # Project the concatenated results\n",
    "        out = self.proj(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        \n",
    "        head_size = n_emb // n_head\n",
    "        \n",
    "        # Multi-Head Attention Layer\n",
    "        self.mul_head = MultiHeadAttention(n_emb, n_head, head_size)\n",
    "        \n",
    "        # Feed-Forward Layer\n",
    "        self.ffwd = FeedFoward(n_emb)\n",
    "        \n",
    "        # Layer Normalization Layers\n",
    "        self.ln1 = nn.LayerNorm(n_emb)\n",
    "        self.ln2 = nn.LayerNorm(n_emb)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-Head Attention Block\n",
    "        x = x + self.mul_head(self.ln1(x))\n",
    "        \n",
    "        # Feed-Forward Block\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.C = nn.Embedding(vocab_size, n_emb)\n",
    "        self.position = nn.Embedding(block_size, n_emb)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head) for _ in range(n_layers)])\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_emb)\n",
    "        \n",
    "        # Linear layer for language modeling\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        \n",
    "    def forward(self, inp, targets):\n",
    "        # Embedding lookup for input data\n",
    "        token_emb = self.C(inp)\n",
    "        position_emb = self.position(torch.arange(inp.shape[1]))\n",
    "        x_emb = token_emb + position_emb\n",
    "        \n",
    "        # Transformer blocks\n",
    "        out = self.blocks(x_emb)\n",
    "        \n",
    "        # Final layer normalization\n",
    "        out = self.ln_f(out)\n",
    "\n",
    "        # Linear layer for language modeling\n",
    "        logits = self.lm_head(out)\n",
    "        \n",
    "        # Reshape logits for the cross-entropy loss\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 15073\n"
     ]
    }
   ],
   "source": [
    "n_layers = 1\n",
    "model = BigramLanguageModel(vocab_size, n_emb, block_size, n_head, n_layers)\n",
    "\n",
    "num_parameters = 0\n",
    "for p in model.parameters():\n",
    "    num_parameters += p.numel()\n",
    "print(\"Total number of trainable parameters:\", num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2123, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    xb, yb = get_batch(data, batch_size, block_size)\n",
    "\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    # Backward\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
