{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2 Architecture (P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chap4/llama_architecture.png\" alt=\"LLAMA2 Architecture\" width=\"400\"/>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong chương này, chúng ta sẽ cùng tìm hiểu về 3 chi tiết còn lại: RMS Norm, Group Multi Query Attention with KV cache, và Feed Forward SwiGLU. Hãy cùng khám phá những khái niệm mới này và tìm hiểu cách chúng hoạt động!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "from einops import rearrange # einstein operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 219])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 20\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "subset_dataset = dataset['train'][:sample]['text']\n",
    "tokenized_dataset = tokenizer(\n",
    "    subset_dataset,\n",
    "    return_tensors='pt',\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True  # Enable truncation\n",
    ")\n",
    "\n",
    "data = tokenized_dataset['input_ids']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs:\n",
    "    def __init__(self, sequence_len, vocab_size):\n",
    "\n",
    "        self.rotary_dim = 3\n",
    "        \n",
    "        self.n_layer = 2\n",
    "        self.batch_size = 16\n",
    "        self.n_head = 4\n",
    "        self.n_embd = 36\n",
    "        self.sequence_len = sequence_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "args = ModelArgs(sequence_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 218]), torch.Size([16, 218]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size):\n",
    "    idx = torch.randint(0, len(data), size=(batch_size,))\n",
    "    batch = data[idx]\n",
    "\n",
    "    xb = batch[:, :-1].contiguous()\n",
    "    yb = batch[:, 1:].contiguous()\n",
    "    \n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(data, args.batch_size)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "\n",
    "<img src=\"images/chap4/llama_embd.png\" alt=\"LLAMA2 Embedding\" width=\"200\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        input_ids_embd = self.wte(input_ids)\n",
    "        \n",
    "        return input_ids_embd\n",
    "    \n",
    "embd = Embedding(args)\n",
    "x_embd = embd(xb)\n",
    "x_embd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Embedding\n",
    "\n",
    "<img src=\"images/chap4/rotary.png\" alt=\"Rotary Position\" width=\"300\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    def __init__(self, args:ModelArgs, base = 10000):\n",
    "        super().__init__()\n",
    "        self.rotary_dim  = args.rotary_dim\n",
    "        \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        self.cos_cache = None\n",
    "        self.sin_cache = None\n",
    "        \n",
    "    def forward(self, qkv):\n",
    "        seqlen = qkv.shape[1]\n",
    "        \n",
    "        # Update cos sin cache\n",
    "        t = torch.arange(seqlen)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        self.cos_cache = torch.cos(freqs)\n",
    "        self.sin_cache = torch.sin(freqs)\n",
    "        \n",
    "        # Apply rotary qkv\n",
    "        rotary_dim = self.cos_cache.shape[1]\n",
    "        rotary_dim *= 2\n",
    "        \n",
    "        q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "        q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "        \n",
    "        k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "        k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "        \n",
    "        # Splits the queries and keys in half\n",
    "        q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "        k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "        c, s = rearrange(self.cos_cache, \"t d -> t 1 d\"), rearrange(self.sin_cache, \"t d -> t 1 d\")\n",
    "        \n",
    "        # Computes the new keys and queries\n",
    "        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n",
    "        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n",
    "        \n",
    "        return torch.cat(\n",
    "            [\n",
    "                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n",
    "                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n",
    "                qkv[:, :, 2:3, :, :]\n",
    "            ],\n",
    "            dim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Norm\n",
    "\n",
    "<img src=\"images/chap4/rms_norm.png\" alt=\"RMS Norm\" width=\"300\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "    \n",
    "attn_norm = RMSNorm(args.n_embd)\n",
    "x_embd_norm = attn_norm(x_embd)\n",
    "x_embd_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMS Norm thực chất là một biến thể của Layer Norm. Ý tưởng cơ bản là thay vì sử dụng Layer Norm làm quá trình normalize dữ liệu, họ chuyển sang sử dụng RMS Norm. Cụ thể, trong ảnh trên thay vì áp dụng layer norm cho x_embd để tính toán qkv, họ thay thế nó bằng RMS Norm. Điều này cũng áp dụng tương tự cho việc normalize output. Sự thay đổi này có thể mang lại một số ưu điểm cụ thể trong quá trình xử lý và huấn luyện mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "<img src=\"images/chap4/self_attention.png\" alt=\"Self Attention\" width=\"300\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rotary_emb = RotaryPositionEmbedding(args)\n",
    "        \n",
    "        self.head_dim = args.n_embd // args.n_head\n",
    "        opt_size = args.n_head * self.head_dim\n",
    "        hidden_size = args.n_embd\n",
    "        \n",
    "        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n",
    "        self.out_proj = nn.Linear(opt_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_ids_embd_norm):\n",
    "        seq_len = input_ids_embd_norm.shape[1]\n",
    "        \n",
    "        qkv = self.Wqkv(input_ids_embd_norm)\n",
    "        qkv = rearrange(qkv, 'b t (three h d) -> b t three h d', three=3, d=self.head_dim)\n",
    "        \n",
    "        # Rotary Query & Key\n",
    "        qkv = self.rotary_emb(qkv)\n",
    "        \n",
    "        q, k, v = qkv.unbind(2)\n",
    "        \n",
    "        # New code\n",
    "        # --------------------------------------------------------------------------------\n",
    "        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        # softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
    "        # scores = torch.einsum(\"bthd, bshd -> bhts\", q, k * softmax_scale)\n",
    "        \n",
    "        # mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n",
    "        # scores += mask\n",
    "        \n",
    "        # attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # output = torch.einsum(\"bhts, bshd -> bthd\", attention_weights, v)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        \n",
    "        output = rearrange(output, \"... h d -> ... (h d)\")\n",
    "\n",
    "        attn_out = self.out_proj(output)\n",
    "        \n",
    "        return attn_out\n",
    "    \n",
    "# Normalize\n",
    "attn_norm = RMSNorm(args.n_embd)\n",
    "x_embd_norm = attn_norm(x_embd)\n",
    "\n",
    "attn = Attention(args)\n",
    "attn_out = attn(x_embd_norm)\n",
    "# add residual\n",
    "attn_out += x_embd\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phương pháp trước đây, việc tính toán lại các scores một cách lặp đi lặp lại dẫn đến sự lãng phí đáng kể về hiệu suất tính toán. Ví dụ, trong câu \"Tôi thích chạy bộ\" khi chúng ta cố gắng dự đoán từ \"thích\" dựa trên từ \"Tôi\", chúng ta thực hiện tính scores bằng cách nhân toàn bộ query của từ cần dự đoán với toàn bộ key của các từ khác, sau đó phải loại bỏ scores của các từ không cần thiết (trong trường hợp này là \"thích chạy bộ\"). Tương tự, khi dự đoán từ \"chạy\", chúng ta lại thực hiện lại quá trình tính scores bằng cách nhân toàn bộ query với các key, sau đó phải loại bỏ scores của các từ không cần thiết (\"chạy bộ\"). Điều này dẫn đến một sự lãng phí lớn về hiệu suất tính toán.\n",
    "\n",
    "Ý tưởng chính của phương pháp \"Group Multi-Query Attention with KV cache\" là giảm thiểu sự lãng phí này bằng cách tận dụng lại các kết quả đã được tính toán và lưu trữ trước đó thay vì tính toán lại từ đầu. Hàm F.scaled_dot_product_attention thực hiện ý tưởng này và đồng thời giúp mã nguồn trở nên rõ ràng hơn đáng kể. Sử dụng cách tiếp cận này giúp chúng ta tái sử dụng các kết quả trước đó đã được tính toán và lưu trữ, từ đó giảm thiểu việc tính toán lại và giúp mã nguồn trở nên dễ đọc và dễ hiểu hơn rất nhiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "<img src=\"images/chap4/feed_forward.png\" alt=\"Feed Forward\" width=\"300\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        multiple_of = 5\n",
    "        \n",
    "        hidden_dim = 4 * args.n_embd\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.w1 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n",
    "        self.w2 = nn.Linear(hidden_dim, args.n_embd, bias=False)\n",
    "        \n",
    "        self.w3 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n",
    "        \n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "    def forward(self, attn_out_norm):\n",
    "        \n",
    "        hidden_states = self.w1(attn_out_norm) * self.w3(attn_out_norm)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        \n",
    "        ffwd_out = self.w2(hidden_states)\n",
    "        \n",
    "        return ffwd_out\n",
    "    \n",
    "# Normalize\n",
    "ffwd_norm = RMSNorm(args.n_embd)\n",
    "attn_out_norm = ffwd_norm(attn_out)\n",
    "\n",
    "ffwd = FeedForward(args)\n",
    "ffwd_out = ffwd(attn_out_norm)\n",
    "# add residual\n",
    "ffwd_out += attn_out\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giống như trước đó, chúng ta sẽ sử dụng RMS Norm để normalize output attention thay vì Layer Norm. \n",
    "\n",
    "Feed Forward SwiGLU là một cải tiến của phương pháp Feed Forward thông thường, nhằm tăng cường khả năng học và biểu diễn của mô hình. Bằng cách tăng cường phức tạp hóa cấu trúc của lớp feed forward, SwiGLU có thể học được các mối quan hệ phức tạp và đặc trưng của dữ liệu một cách hiệu quả hơn. Việc tăng cường tính phức tạp của kiến trúc này thường đi kèm với việc sử dụng các phép tính toán và hàm activation phức tạp hơn (SiLU), nhằm tăng tính linh hoạt và mạnh mẽ của mô hình trong việc xử lý dữ liệu phức tạp và đa dạng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "<img src=\"images/chap4/transformer_block.png\" alt=\"Transformer Block\" width=\"400\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransfomerBlock(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_norm = RMSNorm(args.n_embd)\n",
    "        self.ffwd_norm = RMSNorm(args.n_embd)\n",
    "        \n",
    "        self.attn = Attention(args)\n",
    "        self.ffwd = FeedForward(args)\n",
    "        \n",
    "    def forward(self, input_ids_embd):\n",
    "        \n",
    "        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n",
    "        \n",
    "        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n",
    "        \n",
    "        return ffwd_out\n",
    "    \n",
    "t_block = TransfomerBlock(args)\n",
    "ffwd_out = t_block(x_embd)\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLAMA2 architecture có những cải tiến hơn so với transformer architecture mà chúng ta đã học trước đó. Các cải tiến này chủ yếu tập trung vào các class Attention và Feed Forward. Do đó, các class còn lại cơ bản chỉ thay đổi việc sử dụng normalize từ Layer Norm sang RMS Norm, các code khác đều sẽ  được giữ nguyên."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "<img src=\"images/chap4/transformer.png\" alt=\"Transformer\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 50257])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = RMSNorm(args.n_embd)\n",
    "        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n",
    "        \n",
    "    def forward(self, ffwd_out):\n",
    "        ffwd_out_norm = self.norm(ffwd_out)\n",
    "        logits = self.linear(ffwd_out_norm)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "t_head = TransformerHead(args)\n",
    "logits = t_head(ffwd_out)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 50257])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerSequential(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = [Embedding(args)]\n",
    "        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n",
    "        modules.append(TransformerHead(args))\n",
    "        \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        return self.layers(input_ids)\n",
    "    \n",
    "model = TransformerSequential(args)\n",
    "logits = model(xb)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.8817, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        labels = labels.view(-1)                    \n",
    "                             \n",
    "        loss = self.loss_fct(logits, labels)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "t_loss = TransformerLoss()\n",
    "loss = t_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0817, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenized_dataset['input_ids']\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "args = ModelArgs(sequence_len, vocab_size)\n",
    "xb, yb = get_batch(data, args.batch_size)\n",
    "\n",
    "model = TransformerSequential(args)\n",
    "logits = model(xb)\n",
    "\n",
    "t_loss = TransformerLoss()\n",
    "loss = t_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy là chúng ta đã hoàn thành kiến trúc transformer của LLAMA2.Trong chương tiếp theo, chúng ta sẽ đào sâu vào việc khởi tạo trọng số (weight initialization). Có vẻ như phương pháp khởi tạo trọng số mặc định của PyTorch không còn phù hợp nữa, và LLAMA2 đã sử dụng một phương pháp khởi tạo trọng số khác. Hãy cùng tìm hiểu về điều này trong chương tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
