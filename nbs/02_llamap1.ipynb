{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama2 Architecture (P1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chap3/rotary_position.png\" alt=\"LLAMA2 Architecture\" width=\"400\"/>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong kiến trúc transformers của LLAMA2, có tổng cộng 4 điểm khác biệt chính so với kiến trúc mà chúng ta đã học trước đó. Tuy nhiên, trong chương này, chúng ta sẽ chỉ nói về Rotary Positional Encodings vì có thể chỉ tốn 5 phút nếu bạn chỉ muốn nắm ý tưởng chính của nó, hoặc có thể mất hàng giờ nếu bạn muốn thực sự tìm hiểu sâu hơn về nó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "from einops import rearrange # einstein operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 219])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 20\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "subset_dataset = dataset['train'][:sample]['text']\n",
    "tokenized_dataset = tokenizer(\n",
    "    subset_dataset,\n",
    "    return_tensors='pt',\n",
    "    padding=True,  # Enable padding\n",
    "    truncation=True  # Enable truncation\n",
    ")\n",
    "\n",
    "data = tokenized_dataset['input_ids']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelArgs:\n",
    "    def __init__(self, sequence_len, vocab_size):\n",
    "        \n",
    "        self.n_layer = 2\n",
    "        \n",
    "        self.batch_size = 16\n",
    "        self.n_head = 4\n",
    "        self.n_embd = 36\n",
    "        self.sequence_len = sequence_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "args = ModelArgs(sequence_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 218]), torch.Size([16, 218]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(data, batch_size):\n",
    "    idx = torch.randint(0, len(data), size=(batch_size,))\n",
    "    batch = data[idx]\n",
    "\n",
    "    xb = batch[:, :-1].contiguous()\n",
    "    yb = batch[:, 1:].contiguous()\n",
    "    \n",
    "    return xb, yb\n",
    "\n",
    "xb, yb = get_batch(data, args.batch_size)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chap3/llama_embd.png\" alt=\"LLAMA2 Embedding\" width=\"200\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        input_ids_embd = self.wte(input_ids)\n",
    "        \n",
    "        return input_ids_embd\n",
    "    \n",
    "embd = Embedding(args)\n",
    "x_embd = embd(xb)\n",
    "x_embd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần Embedding, thay vì sử dụng phần Position Embedding như trước, LLAMA2 sử dụng phần \"rotary position embedding\" mà chúng ta sẽ đề cập ở phần dưới."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Position Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/chap3/rotary_position.png\" alt=\"Rotary Position\" width=\"300\"/>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionEmbedding(nn.Module):\n",
    "    def __init__(self, args:ModelArgs, base = 10000):\n",
    "        super().__init__()\n",
    "        self.rotary_dim  = 3\n",
    "        \n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        self.cos_cache = None\n",
    "        self.sin_cache = None\n",
    "        \n",
    "    def forward(self, qkv):\n",
    "        seqlen = qkv.shape[1]\n",
    "        \n",
    "        # Update cos sin cache\n",
    "        t = torch.arange(seqlen)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        self.cos_cache = torch.cos(freqs)\n",
    "        self.sin_cache = torch.sin(freqs)\n",
    "        \n",
    "        # Apply rotary qkv\n",
    "        rotary_dim = self.cos_cache.shape[1]\n",
    "        rotary_dim *= 2\n",
    "        \n",
    "        q_rot = qkv[:, :, 0, :, :rotary_dim]\n",
    "        q_pass = qkv[:, :, 0, :, rotary_dim:]\n",
    "        \n",
    "        k_rot = qkv[:, :, 1, :, :rotary_dim]\n",
    "        k_pass = qkv[:, :, 1, :, rotary_dim:]\n",
    "        \n",
    "        # Splits the queries and keys in half\n",
    "        q1, q2 = q_rot.chunk(2, dim=-1)\n",
    "        k1, k2 = k_rot.chunk(2, dim=-1)\n",
    "        c, s = rearrange(self.cos_cache, \"t d -> t 1 d\"), rearrange(self.sin_cache, \"t d -> t 1 d\")\n",
    "        \n",
    "        # Computes the new keys and queries\n",
    "        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n",
    "        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n",
    "        \n",
    "        return torch.cat(\n",
    "            [\n",
    "                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n",
    "                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n",
    "                qkv[:, :, 2:3, :, :]\n",
    "            ],\n",
    "            dim=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rotary Position Embedding, là một phiên bản tối ưu hóa của việc Position Embedding thông thường. Thay vì đơn giản là thêm một vector vị trí cho từng từ trong câu, chúng ta thực hiện một phép xoay (rotation) trên các giá trị trong ma trận Query và ma trận Key.\n",
    "\n",
    "Mục tiêu chính của Rotary Position Embedding là tối ưu hóa tốc độ tính toán. Bằng cách áp dụng phép xoay này, chúng ta giúp mô hình học cách tương tác vị trí một cách hiệu quả hơn và giảm sự phức tạp của tính toán so với việc sử dụng Position Embedding truyền thống bằng vector vị trí.\n",
    "\n",
    "Đoạn code trên được dựa trên code của [Microsoft Phi-1.5](https://huggingface.co/microsoft/phi-1_5/blob/main/modeling_mixformer_sequential.py). Tôi thấy đoạn code này gọn gàng và dễ hiểu hơn, vì vậy tôi đã sử dụng nó. Tuy nhiên, về bản chất, không có sự khác biệt lớn so với code của LLAMA2.\n",
    "\n",
    "Trong chương này chúng ta sẽ tạm dừng tại đây. Tôi sẽ chỉ thêm một dòng code mới để thực hiện phép xoay QKV trong class Attention. Còn lại, tôi sẽ giữ nguyên như chương trước. Lý do là vì tôi nghĩ sẽ có nhiều bạn muốn thực sự hiểu toàn bộ mã code ở trên và sẽ cần dành nhiều thời gian để nghiên cứu. Và nếu như vậy, chương này đã là quá dài và nên tạm dừng để họ có thể giải lao và chuyển sang một chương mới vào ngày hôm sau. Tuy nhiên nếu bạn giống tôi, sẵn sàng tiếp tục học kiến thức mới sau khi đã hiểu ý tưởng cơ bản của Rotary Position Embedding, hãy cứ thoải mái mà chuyển sang chương tiếp theo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rotary_emb = RotaryPositionEmbedding(args)\n",
    "        \n",
    "        self.head_dim = args.n_embd // args.n_head\n",
    "        opt_size = args.n_head * self.head_dim\n",
    "        hidden_size = args.n_embd\n",
    "        \n",
    "        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n",
    "        self.out_proj = nn.Linear(opt_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input_ids_embd_norm):\n",
    "        seq_len = input_ids_embd_norm.shape[1]\n",
    "        \n",
    "        qkv = self.Wqkv(input_ids_embd_norm)\n",
    "        qkv = rearrange(qkv, 'b t (three h d) -> b t three h d', three=3, d=self.head_dim)\n",
    "        \n",
    "        # New code\n",
    "        # Rotary Query & Key\n",
    "        # -------------------------\n",
    "        qkv = self.rotary_emb(qkv)\n",
    "        # -------------------------\n",
    "        \n",
    "        q, k, v = qkv.unbind(2)\n",
    "        \n",
    "        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n",
    "        scores = torch.einsum(\"bthd, bshd -> bhts\", q, k * softmax_scale)\n",
    "        \n",
    "        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n",
    "        scores += mask\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        output = torch.einsum(\"bhts, bshd -> bthd\", attention_weights, v)\n",
    "        output = rearrange(output, \"... h d -> ... (h d)\")\n",
    "\n",
    "        attn_out = self.out_proj(output)\n",
    "        \n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "attn_norm = nn.LayerNorm(args.n_embd)\n",
    "x_embd_norm = attn_norm(x_embd)\n",
    "\n",
    "attn = Attention(args)\n",
    "attn_out = attn(x_embd_norm)\n",
    "# add residual\n",
    "attn_out += x_embd\n",
    "attn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        hidden_size = 4 * args.n_embd\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.n_embd, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, args.n_embd)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, attn_out_norm):\n",
    "        hidden_states = self.fc1(attn_out_norm)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        ffwd_out = self.fc2(hidden_states)\n",
    "        \n",
    "        return ffwd_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "ffwd_norm = nn.LayerNorm(args.n_embd)\n",
    "attn_out_norm = ffwd_norm(attn_out)\n",
    "\n",
    "ffwd = FeedForward(args)\n",
    "ffwd_out = ffwd(attn_out_norm)\n",
    "# add residual\n",
    "ffwd_out += attn_out\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransfomerBlock(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_norm = nn.LayerNorm(args.n_embd)\n",
    "        self.ffwd_norm = nn.LayerNorm(args.n_embd)\n",
    "        \n",
    "        self.attn = Attention(args)\n",
    "        self.ffwd = FeedForward(args)\n",
    "        \n",
    "    def forward(self, input_embd):\n",
    "        \n",
    "        attn_out = input_embd + self.attn(self.attention_norm(input_embd))\n",
    "        \n",
    "        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n",
    "        \n",
    "        return ffwd_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 36])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_block = TransfomerBlock(args)\n",
    "ffwd_out = t_block(x_embd)\n",
    "ffwd_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 50257])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(args.n_embd)\n",
    "        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n",
    "        \n",
    "    def forward(self, ffwd_out):\n",
    "        ffwd_out_norm = self.norm(ffwd_out)\n",
    "        logits = self.linear(ffwd_out_norm)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "t_head = TransformerHead(args)\n",
    "logits = t_head(ffwd_out)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSequential(nn.Module):\n",
    "    def __init__(self, args:ModelArgs):\n",
    "        super().__init__()\n",
    "        modules = [Embedding(args)]\n",
    "        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n",
    "        modules.append(TransformerHead(args))\n",
    "        \n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        logits = self.layers(input_ids)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 218, 50257])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerSequential(args)\n",
    "logits = model(xb)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        labels = labels.view(-1)                    \n",
    "                             \n",
    "        loss = self.loss_fct(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7649, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_loss = TransformerLoss()\n",
    "loss = t_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.2564, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenized_dataset['input_ids']\n",
    "sequence_len = data.size(1) - 1\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "args = ModelArgs(sequence_len, vocab_size)\n",
    "xb, yb = get_batch(data, args.batch_size)\n",
    "\n",
    "model = TransformerSequential(args)\n",
    "logits = model(xb)\n",
    "\n",
    "t_loss = TransformerLoss()\n",
    "loss = t_loss(logits, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
