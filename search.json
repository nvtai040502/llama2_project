[
  {
    "objectID": "Appendix/transformer.html",
    "href": "Appendix/transformer.html",
    "title": "Transformer Architecture",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "Appendix/transformer.html#the-goal",
    "href": "Appendix/transformer.html#the-goal",
    "title": "Transformer Architecture",
    "section": "The Goal",
    "text": "The Goal\nTrong chương này, mục tiêu hàng đầu của chúng ta là khám phá một cách chi tiết và cụ thể từng bước của quá trình giải mã (decoder) (phần được khoanh đỏ) dựa trên kiến trúc Attention, như hình minh họa dưới đây:\n\nChúng ta sẽ đảm bảo rằng mỗi bước trong quy trình này được diễn giải một cách chi tiết để chúng ta có thể hiểu sâu hơn về cách nó hoạt động và tương tác với dữ liệu đầu vào. Điều quan trọng là thông qua việc làm này, chúng ta sẽ có cơ hội thấu hiểu rõ hơn về cách áp dụng kiến thức này vào các dự án thực tế, ví dụ như xây dựng một Mô hình Ngôn ngữ Lớn (Large Language Model) cho tiếng Việt, mở ra nhiều tiềm năng ứng dụng hấp dẫn."
  },
  {
    "objectID": "Appendix/transformer.html#tiny-shakespeare",
    "href": "Appendix/transformer.html#tiny-shakespeare",
    "title": "Transformer Architecture",
    "section": "Tiny Shakespeare",
    "text": "Tiny Shakespeare\n\nwith open('data/input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\nprint(type(text))    \nprint(\"length of dataset in characters: \", len(text))\n\n&lt;class 'str'&gt;\nlength of dataset in characters:  1115394\n\n\nTập dữ liệu “tiny Shakespeare” là một kho văn bản chứa các tác phẩm của danh tác William Shakespeare, với hơn 1 triệu ký tự. Mục tiêu chính của việc sử dụng tập dữ liệu này là xây dựng một mô hình mạng neural có khả năng dự đoán ký tự tiếp theo trong một đoạn văn dựa trên các ký tự trước đó. Mô hình này sẽ có khả năng tái tạo cấu trúc và phong cách viết của Shakespeare, tạo ra văn bản một cách tự nhiên và đầy hấp dẫn.\n\n# let's look at the first 200 characters\nprint(text[:200])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you"
  },
  {
    "objectID": "Appendix/transformer.html#process-data",
    "href": "Appendix/transformer.html#process-data",
    "title": "Transformer Architecture",
    "section": "Process Data",
    "text": "Process Data\n\n# Embedding\nchars = sorted(set(text))\nprint(''.join(chars))\n\nvocab_size = len(chars)\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\nTrong dự án này, chúng ta sẽ sử dụng một phương pháp nhúng (embedding) đơn giản. Cụ thể, chúng ta sẽ xác định tất cả các ký tự duy nhất có trong toàn bộ tập dữ liệu và gán một số duy nhất cho mỗi ký tự này.\nTrong tập dữ liệu của chúng ta, có tổng cộng 65 ký tự khác nhau. Mục tiêu chính của dự án là xây dựng một mô hình có khả năng dự đoán ký tự tiếp theo nằm trong 65 ký tự này. Điều này có nghĩa rằng chúng ta muốn mô hình học cách dự đoán ký tự tiếp theo dựa trên ngữ cảnh và phân tích các ký tự trước đó trong chuỗi văn bản.\n\n# Character Encoding\nstoi = {s:i for i, s in enumerate(chars)}\nitos = {i:s for s, i in stoi.items()}\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l:  ''.join([itos[i] for i in l])\n\nỞ đây, chúng ta đang thực hiện một quá trình được gọi là “mã hóa ký tự (character encoding)”. Trong quá trình này, mỗi ký tự riêng biệt trong dữ liệu của chúng ta sẽ được ánh xạ thành một số nguyên tương ứng. Chúng ta thực hiện việc này để có khả năng chuyển đổi linh hoạt giữa chuỗi ký tự và số nguyên, giúp chúng ta hiểu và biểu diễn kết quả một cách dễ dàng và hiệu quả hơn.\n\ntext_exp = \"Hello World\"\nprint(encode(text_exp))\nprint(decode(encode(text_exp)))\n\n[20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]\nHello World\n\n\n\n# Convert all text data to integers\ndata = torch.tensor(encode(text), dtype = torch.long)\ndata[:16]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14])\n\n\nChuyển mọi ký tự trong dữ liệu “Tiny Shakespeare” sang số nguyên.\n\n# Define block size and batch size\nblock_size = 8\nbatch_size = 4\n\n# Generate random indices within the valid range\nix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n# Extract blocks of data using the generated indices\nxb = torch.stack([data[i:i+block_size] for i in ix])\n\n# Extract corresponding target blocks\n# Note that yb is reshaped to a 1D tensor\nyb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n\n# Print the shapes of xb and yb\nprint(\"xb.shape:\", xb.shape)\nprint(\"yb.shape:\", yb.shape)\n\nxb.shape: torch.Size([4, 8])\nyb.shape: torch.Size([32])\n\n\nBên trên là một ví dụ minh họa về cách tạo x_batch và y_batch sử dụng batch_size, trong đó x_batch được cố định theo block size.\n\nfor i in range(block_size):\n    inp = xb[0, :i+1].tolist()\n    target = yb[i]\n    print(f\"Input: {inp} --&gt; Target: {target}\")\n\nInput: [58] --&gt; Target: 46\nInput: [58, 46] --&gt; Target: 39\nInput: [58, 46, 39] --&gt; Target: 58\nInput: [58, 46, 39, 58] --&gt; Target: 1\nInput: [58, 46, 39, 58, 1] --&gt; Target: 58\nInput: [58, 46, 39, 58, 1, 58] --&gt; Target: 46\nInput: [58, 46, 39, 58, 1, 58, 46] --&gt; Target: 43\nInput: [58, 46, 39, 58, 1, 58, 46, 43] --&gt; Target: 56"
  },
  {
    "objectID": "Appendix/transformer.html#hyperparameters",
    "href": "Appendix/transformer.html#hyperparameters",
    "title": "Transformer Architecture",
    "section": "Hyperparameters",
    "text": "Hyperparameters\n\nlearning_rate = 1e-3\nn_epochs = 1500\n\nvocab_size = len(chars)\nn_emb = 32\n\nbatch_size = 64\nblock_size = 8\n\nhead_size = 20\nn_head = 4\n\nĐể tránh sự lặp lại không cần thiết trong quá trình giải thích cách thực hiện, chúng ta sẽ duy trì liên tục các biến sau đây."
  },
  {
    "objectID": "Appendix/transformer.html#distributed-presentation",
    "href": "Appendix/transformer.html#distributed-presentation",
    "title": "Transformer Architecture",
    "section": "Distributed Presentation",
    "text": "Distributed Presentation\n\n# Define the embedding size\nC = torch.randn(vocab_size, n_emb)\n\nweight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\nbias = torch.zeros(vocab_size)\n\nparameters = [C, weight, bias]\n\nnum_parameters = 0\nfor p in parameters:\n    p.requires_grad = True\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 4225\n\n\n\n# Forward sample\nx_emb = C[xb]  # Embedding lookup for input data\nprint(\"x_emb.shape:\", x_emb.shape)\n\n# Compute logits using a linear transformation\nlogits = x_emb @ weight + bias\nprint(\"logits.shape:\", logits.shape)\n\n# Reshape logits for the cross-entropy loss\nlogits = logits.view(-1, logits.shape[-1])\n\n# Compute the cross-entropy loss\nloss = F.cross_entropy(logits, yb)\nprint(\"loss:\", loss)\n\nx_emb.shape: torch.Size([4, 8, 32])\nlogits.shape: torch.Size([4, 8, 65])\nloss: tensor(4.7028, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\noptimizer = torch.optim.AdamW(parameters, lr = learning_rate)\n\nfor epochi in range(n_epochs):\n    # Generate random indices within the valid range\n    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n    # Extract blocks of data using the generated indices\n    xb = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Extract corresponding target blocks\n    # Note that yb is reshaped to a 1D tensor\n    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n\n    x_emb = C[xb]  # Embedding lookup for input data\n\n    # Compute logits using a linear transformation\n    logits = x_emb @ weight + bias\n\n    # Reshape logits for the cross-entropy loss\n    logits = logits.view(-1, logits.shape[-1])\n\n    # Compute the cross-entropy loss\n    loss = F.cross_entropy(logits, yb)\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.4565, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "Appendix/transformer.html#attention-mechanism",
    "href": "Appendix/transformer.html#attention-mechanism",
    "title": "Transformer Architecture",
    "section": "Attention Mechanism",
    "text": "Attention Mechanism\n\n1. Position\n\nC = torch.randn(vocab_size, n_emb)\n\nweight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\nbias = torch.zeros(vocab_size)\n\n# New code\n# ------------------------------------------------------------\nposition = torch.randn(block_size, n_emb) * block_size **-0.5\nparameters = [C, weight, bias, position]\n# ------------------------------------------------------------\n\nnum_parameters = 0\nfor p in parameters:\n    p.requires_grad = True\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 4481\n\n\nTrong phương pháp “Distributed Presentation,” ban đầu chúng ta đã sử dụng việc nhúng (embedding) để ánh xạ từng từ vào một vector đặc trưng riêng biệt (matrix C). Tuy nhiên, để nâng cao khả năng biểu diễn, chúng ta muốn không chỉ biết về vector đặc trưng (C) của ký tự mà còn quan tâm đến vị trí (position) của ký tự đó trong câu.\nĐể thực hiện điều này, chúng ta sẽ tạo ra một ma trận vị trí mới (matrix position). Trong ma trận này, mỗi hàng sẽ tương ứng với một vị trí trong câu và nó sẽ được sử dụng để kết hợp với vector đặc trưng (C) của ký tự tại vị trí tương ứng, giúp cải thiện khả năng biểu diễn của mô hình.\n\ndef get_batch(data, batch_size, block_size):\n    # Generate random indices within the valid range\n    ix = torch.randint(0, len(data) - block_size, size=(batch_size,))\n\n    # Extract blocks of data using the generated indices\n    xb = torch.stack([data[i:i+block_size] for i in ix])\n\n    # Extract corresponding target blocks\n    # Note that yb is reshaped to a 1D tensor\n    yb = torch.stack([data[i+1:i+block_size+1] for i in ix]).view(-1)\n    \n    return xb, yb\n\nxb, yb = get_batch(data, batch_size, block_size)\nxb.shape, yb.shape\n\n(torch.Size([64, 8]), torch.Size([512]))\n\n\nĐể tránh việc lặp lại mã code và để tạo sự tiện lợi, tôi tạo một hàm có tên là get_batch để tự động tạo các batch x và y dựa trên kích thước batch_size và block_size.\n\noptimizer = torch.optim.AdamW(parameters, lr = learning_rate)\n\nfor i in range(n_epochs):\n    # New code\n    # -----------------------------------------------\n    xb, yb = get_batch(data, batch_size, block_size)\n    \n    # Embedding lookup for input data\n    x_emb = C[xb]  \n    x_emb += position\n    # -----------------------------------------------\n    \n    # Compute logits using a linear transformation\n    logits = x_emb @ weight + bias\n\n    # Reshape logits for the cross-entropy loss\n    logits = logits.view(-1, logits.shape[-1])\n\n    # Compute the cross-entropy loss\n    loss = F.cross_entropy(logits, yb)\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.4990, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n2. Weight Average\nHiện tại, mô hình chỉ dựa vào ký tự và vị trí gần nhất của ký tự đó để thực hiện dự đoán. Tuy nhiên, điều này không đủ hiệu quả. Chúng ta muốn mô hình có khả năng sử dụng tất cả thông tin từ các ký tự trước đó để cải thiện dự đoán ký tự tiếp theo.\nHãy xem xét ví dụ từ hai từ “his” và “like.” Giả sử chúng ta cung cấp cho mô hình các vector đặc trưng biểu diễn cho từ “i” và vị trí thứ hai của từ “i” trong câu. Tuy nhiên, trong trường hợp này, mô hình có thể dự đoán ký tự tiếp theo là “s” hoặc “k” mà không có thông tin đủ để quyết định. Điều quan trọng là chúng ta cần cung cấp cho mô hình thông tin về ký tự “h” đứng trước ký tự “i” thay vì “l” để mô hình có thể học được và dự đoán đúng ký tự “s” là ký tự tiếp theo.\n\n# Lower triangular matrix for masking\ntril = torch.tril(torch.ones(block_size, block_size))\n\n# Masking to make sure the network can't attend to the future positions\nwei = tril.masked_fill(tril==0, float('-inf'))\n\n# Applying softmax to get the attention probabilities\nwei = F.softmax(wei, dim=-1)\nprint(wei)\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\nDo đó, chiến lược tạm thời của chúng ta ở đây là tích hợp thông tin từ tất cả các ký tự trước đó, đã xuất hiện, bằng cách tính trung bình của các vector biểu diễn và vị trí của chúng. Điều này sẽ giúp mô hình dự đoán từ tiếp theo một cách chính xác hơn.\n\nC = torch.randn(vocab_size, n_emb)\nposition = torch.randn(block_size, n_emb)\n\nx_emb = C[xb]\nx_emb +=  position\n\n# New code\n# -------------------------------------------------------\n# Lower triangular matrix for masking\ntril = torch.tril(torch.ones(block_size, block_size))\n\n# Masking to make sure the network can't attend to the future positions\nwei = tril.masked_fill(tril==0, float('-inf'))\n\n# Applying softmax to get the attention probabilities\nwei = F.softmax(wei, dim=-1)\n\nout = wei @ x_emb\n# --------------------------------------------------------\n\n# Print the first 3 elements of the original input and the transformed input for the first block\nprint(f\"Original Input (first batch, first 3 characters):\\n {x_emb[0, :3]}\")\nprint(\"\")\nprint(f\"Transformed Input (first batch, first 3 characters):\\n {out[0, :3]}\")\n\nOriginal Input (first batch, first 3 characters):\n tensor([[-0.4335,  0.2487, -1.6515,  2.6138,  0.4584,  1.0921, -1.7675, -0.7268,\n          2.0422,  2.2848,  0.9093,  0.6099,  1.3877, -0.3385, -3.3128, -0.5631,\n         -0.7516,  0.2281, -0.5588,  1.6799, -0.8824,  0.6582,  0.7420, -0.1393,\n         -1.3523, -0.5295,  0.8053, -1.7366, -0.1735, -0.6012, -1.0308, -1.0555],\n        [-1.0184, -2.0927, -0.3102,  3.6519, -2.4072,  0.0139,  0.9342, -1.7704,\n         -0.1125, -0.5072, -1.2626, -1.7496, -1.1825,  0.0487, -0.9318,  0.1392,\n         -2.3752,  1.4060, -1.2250,  1.9381,  0.3784, -1.2098,  0.6793,  0.8746,\n         -0.5673, -3.0030,  1.0940, -1.0829,  0.0083,  2.4880, -0.3996,  2.7292],\n        [ 0.4108,  0.8699, -1.0485, -3.0167,  0.0901, -0.1466, -0.6756,  0.8492,\n          0.2072,  1.3359, -0.2287,  1.1866, -1.0809, -1.1253, -0.8090, -1.1819,\n         -2.4869, -3.1272,  0.2352,  0.4746,  0.2054, -1.5705, -1.3706, -0.6790,\n          0.5017, -2.9568,  0.0115, -0.3515,  1.4428, -1.3596, -0.5216,  0.8348]])\n\nTransformed Input (first batch, first 3 characters):\n tensor([[-0.4335,  0.2487, -1.6515,  2.6138,  0.4584,  1.0921, -1.7675, -0.7268,\n          2.0422,  2.2848,  0.9093,  0.6099,  1.3877, -0.3385, -3.3128, -0.5631,\n         -0.7516,  0.2281, -0.5588,  1.6799, -0.8824,  0.6582,  0.7420, -0.1393,\n         -1.3523, -0.5295,  0.8053, -1.7366, -0.1735, -0.6012, -1.0308, -1.0555],\n        [-0.7260, -0.9220, -0.9808,  3.1329, -0.9744,  0.5530, -0.4166, -1.2486,\n          0.9648,  0.8888, -0.1766, -0.5698,  0.1026, -0.1449, -2.1223, -0.2119,\n         -1.5634,  0.8171, -0.8919,  1.8090, -0.2520, -0.2758,  0.7107,  0.3676,\n         -0.9598, -1.7663,  0.9496, -1.4098, -0.0826,  0.9434, -0.7152,  0.8369],\n        [-0.3471, -0.3247, -1.0034,  1.0830, -0.6196,  0.3198, -0.5029, -0.5493,\n          0.7123,  1.0379, -0.1940,  0.0157, -0.2919, -0.4717, -1.6845, -0.5353,\n         -1.8712, -0.4977, -0.5162,  1.3642, -0.0995, -0.7074,  0.0169,  0.0187,\n         -0.4726, -2.1631,  0.6369, -1.0570,  0.4259,  0.1757, -0.6507,  0.8362]])\n\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Embedding layers\n        self.C = torch.randn(vocab_size, n_emb) * vocab_size ** -0.5\n        self.position = torch.randn(block_size, n_emb) * block_size **-0.5\n        \n        # Linear layer for language modeling\n        self.weight = torch.randn(n_emb, vocab_size) * n_emb **-0.5\n        self.bias = torch.zeros(vocab_size)\n        \n        self.parameters = [self.C, self.weight, self.bias, self.position]\n        for p in self.parameters:\n            p.requires_grad = True\n\n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        x_emb = self.C[inp] \n        x_emb += position\n        \n        # New code\n        # -------------------------------------------------------------\n        # Lower triangular matrix for masking\n        tril = torch.tril(torch.ones(block_size, block_size))\n        \n        # Masking to make sure the network can't attend to the future positions\n        wei = tril.masked_fill(tril==0, float('-inf'))\n        \n        # Applying softmax to get the attention probabilities\n        wei = F.softmax(wei, dim=1)\n        \n        out = wei @ x_emb\n        logits = out @ weight + bias\n        # -------------------------------------------------------------\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters:\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 4481\n\n\n\noptimizer = torch.optim.AdamW(model.parameters, lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(3.0674, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n3. Key, Query, Value\nMô hình hiện tại của chúng ta vẫn chưa đủ hiệu quả, vì chúng ta cần xem xét xác suất quan trọng của các từ đã xuất hiện trước đó đối với việc dự đoán ký tự tiếp theo. Hãy xem xét ví dụ với các từ thay vì ký tự, vì tôi nghĩ điều này có thể giúp chúng ta hiểu rõ hơn.\nVí dụ, trong câu “He is a boy,” để dự đoán từ “boy,” các từ “he” và “is” sẽ có đóng góp quan trọng hơn so với từ “a” trong quá trình dự đoán. Điều này có nghĩa là mô hình cần hiểu được sự liên kết ngữ cảnh giữa các từ và xác định xem từ nào có ảnh hưởng lớn đến dự đoán của mình.\n\n# Get a batch of data\nxb, yb = get_batch(data, batch_size, block_size)\n\nC = torch.randn(vocab_size, n_emb)\nposition = torch.randn(block_size, n_emb)\n# Embed input data\nx_emb = C[xb]\nx_emb += position\n\n# New code\n# ------------------------------------------------------\n# Initialize key and query matrices\nkey = torch.randn(n_emb, head_size) * n_emb ** -0.5\nquery = torch.randn(n_emb, head_size) * n_emb ** -0.5\n\n# Calculate the key and query values\nk = x_emb @ key\nq = x_emb @ query\n\n# Compute the dot product between queries and keys\nwei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\nprint(wei.shape)\n\ntril = torch.tril(torch.ones(block_size, block_size))\nwei = wei.masked_fill(tril==0, float('-inf')) \n# ------------------------------------------------------\n\nwei=F.softmax(wei, dim=-1)\nprint(wei[0])\n\ntorch.Size([64, 8, 8])\ntensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [1.0000e+00, 4.5888e-07, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [2.2766e-02, 9.3893e-01, 3.8305e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [4.0015e-03, 8.9893e-01, 3.7121e-05, 9.7027e-02, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [1.4366e-02, 1.1882e-03, 6.8691e-01, 1.9523e-02, 2.7801e-01, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [1.7339e-15, 1.4864e-16, 9.9915e-01, 8.5241e-04, 5.7747e-15, 1.8133e-14,\n         0.0000e+00, 0.0000e+00],\n        [3.1721e-01, 1.3526e-06, 3.7937e-09, 2.6322e-06, 1.0903e-10, 1.1801e-01,\n         5.6478e-01, 0.0000e+00],\n        [6.3539e-10, 5.0136e-11, 9.9992e-01, 7.7084e-05, 2.7238e-08, 1.9036e-14,\n         2.8848e-12, 4.0533e-11]])\n\n\n\n# New code\n# ------------------------------------------------------\nvalue = torch.randn(n_emb, head_size) * n_emb ** -0.5\nv = x_emb @ value\n\nout = wei @ v\n# ------------------------------------------------------\n\nout.shape\n\ntorch.Size([64, 8, 20])\n\n\nHãy tưởng tượng rằng bạn là một nhà báo nổi tiếng đang thực hiện một cuộc phỏng vấn với một ngôi sao nổi tiếng, và bạn muốn thu thập thông tin quan trọng từ cuộc trò chuyện đó.\n\nKey có thể coi như danh sách câu hỏi bạn chuẩn bị trước cuộc phỏng vấn. Mỗi câu hỏi là một Key, và mỗi câu hỏi sẽ tập trung vào một khía cạnh cụ thể của cuộc trò chuyện. Ví dụ, một Key có thể là “Bạn đã từng giành giải Oscar chưa?”\nValue là câu trả lời mà ngôi sao đưa ra cho từng câu hỏi. Mỗi câu trả lời chứa thông tin quan trọng về cuộc trò chuyện, và nó sẽ được lưu trữ và sử dụng sau này khi bạn cần nắm bắt thông tin cụ thể từ cuộc phỏng vấn. Chúng ta có thể coi câu trả lời này là “value” của câu hỏi.\nQuery là cách bạn đặt câu hỏi hoặc tìm kiếm thông tin trong cuộc phỏng vấn. Khi bạn muốn biết điều gì đó cụ thể hoặc muốn nắm bắt một thông tin quan trọng từ cuộc trò chuyện, bạn sẽ đặt câu hỏi hoặc tạo một “Query” riêng. Ví dụ, “Giới thiệu về những vai diễn nổi bật nhất của bạn?” có thể là một Query.\n\nKhi bạn đặt một câu hỏi (Query), mô hình sẽ so sánh nó với danh sách các câu hỏi trước đó (Key) và quyết định câu trả lời nào (Value) chứa thông tin phù hợp nhất với câu hỏi của bạn. Điều này giống như việc bạn tập trung vào câu hỏi cụ thể nào trong cuộc trò chuyện để thu thập thông tin bạn cần.\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.weight1 = torch.randn(n_emb, 3 * n_emb) * n_emb ** -0.5\n        self.bias1 = torch.zeros(3 * n_emb)\n        \n        self.weight2 = torch.randn(3 * n_emb, n_emb) * ((3 * n_emb) ** -0.5)\n        self.bias2 = torch.zeros(n_emb)\n\n        self.parameters = [self.weight1, self.bias1, self.weight2, self.bias2]\n        \n    def forward(self, x):\n        x = x @ self.weight1 + self.bias1\n        x = F.relu(x)\n        out = x @ self.weight2 + self.bias2\n        \n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Embedding layers\n        self.C = torch.randn(vocab_size, n_emb) * vocab_size ** -0.5\n        self.position = torch.randn(block_size, n_emb) * block_size **-0.5\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n        # New code\n        # ---------------------------------------------------------------\n        self.proj = torch.randn(head_size, n_emb) * head_size ** -0.5\n        \n        self.key = torch.randn(n_emb, head_size) * n_emb ** -0.5\n        self.query = torch.randn(n_emb, head_size) * n_emb ** -0.5\n        self.value = torch.randn(n_emb, head_size) * n_emb ** -0.5\n\n        self.parameters = [self.C, self.position, self.key, self.query, \\\n                           self.value, self.proj] + self.ffwd.parameters\n        # ---------------------------------------------------------------\n\n        for p in self.parameters:\n            p.requires_grad = True\n    \n    def forward(self, inp, targets):\n        x_emb = self.C[inp]  # Embedding lookup for input data\n        x_emb += position\n\n        # New code\n        # -----------------------------------------------------\n        k = x_emb @ self.key\n        q = x_emb @ self.query\n\n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        \n        tril = torch.tril(torch.ones(block_size, block_size))\n        wei = wei.masked_fill(tril==0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        v = x_emb @ self.value\n        out = wei @ v\n        \n        out = out @ self.proj\n        \n        # Feed-Forward\n        out = self.ffwd(out)\n        # ----------------------------------------------------\n\n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters:\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 11168\n\n\n\noptimizer = torch.optim.AdamW(model.parameters, lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.9821, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n4. Layer Norm\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.inp = nn.Linear(n_emb, 3 * n_emb)\n        self.fc1 = nn.Linear(3 * n_emb, n_emb)\n        \n    def forward(self, x):\n        x = self.inp(x)\n        x = F.relu(x)\n        out = self.fc1(x) \n        return out\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # Single-Head Attention Layer\n        self.head = Head(head_size)\n        self.proj = nn.Linear(head_size, n_emb)\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n        # New code\n        # ------------------------------------\n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n        self.ln3 = nn.LayerNorm(n_emb)\n        # ------------------------------------\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.size(1)))\n        x_emb = token_emb + position_emb\n\n        # New code\n        # -----------------------------------------------------\n        # Single-Head Attention\n        out = self.head(self.ln1(x_emb))\n        out = self.proj(out)\n        \n        # Feed-Forward\n        out = self.ffwd(self.ln2(out))\n\n        # Final layer normalization\n        out = self.ln3(out)\n        # ----------------------------------------------------\n        \n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 13537\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2237, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n5. Multi-head Attention\n\n# Get a batch of data\nxb, yb = get_batch(data, batch_size, block_size)\n\nC = torch.randn(vocab_size, n_emb)\nposition = torch.randn(block_size, n_emb)\n# Embed input data\nx_emb = C[xb]\nx_emb += position\n\n# New code\n# ----------------------------------------------------------------------\n# Initialize key and query matrices\nkey_list = [torch.randn(n_emb, head_size // n_head) * \\\n            n_emb ** -0.5 for headi in range(n_head)]\nquery_list = [torch.randn(n_emb, head_size // n_head) * \\\n              n_emb ** -0.5 for headi in range(n_head)]\n\n# Calculate the key and query values\nk = torch.stack([x_emb @ key for key in key_list], dim = -1).view\\\n                                    (xb.shape[0], block_size, -1)\nq = torch.stack([x_emb @ query for query in query_list], dim = -1).view\\\n                                    (xb.shape[0], block_size, -1)\n# ----------------------------------------------------------------------\n\nprint(k.shape)\nprint(q.shape)\n\n# Compute the dot product between queries and keys\nwei = q @ k.transpose(-2, -1)\nprint(wei.shape)\n\ntril = torch.tril(torch.ones(block_size, block_size))\nwei = wei.masked_fill(tril==0, float('-inf')) \nwei = F.softmax(wei, dim=-1)\n\nprint(wei[0])\n\ntorch.Size([64, 8, 20])\ntorch.Size([64, 8, 20])\ntorch.Size([64, 8, 8])\ntensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [2.7624e-02, 9.7238e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [6.3034e-07, 9.9980e-01, 2.0172e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [8.8760e-07, 9.9450e-01, 5.5003e-03, 3.7328e-10, 0.0000e+00, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [6.4108e-01, 2.2718e-05, 4.7551e-02, 3.0472e-01, 6.6321e-03, 0.0000e+00,\n         0.0000e+00, 0.0000e+00],\n        [3.5899e-09, 2.0306e-06, 1.2872e-01, 1.9127e-11, 9.4477e-13, 8.7128e-01,\n         0.0000e+00, 0.0000e+00],\n        [3.4409e-09, 2.2824e-02, 8.0748e-01, 4.0223e-02, 5.4004e-03, 1.2398e-01,\n         9.4144e-05, 0.0000e+00],\n        [1.4672e-07, 9.9970e-01, 3.0362e-04, 2.6681e-13, 2.8710e-12, 1.0302e-06,\n         2.3637e-08, 2.4413e-09]])\n\n\n\n# New code\n# ----------------------------------------------------------------------\nvalue_list = [torch.randn(n_emb, head_size // n_head) * \\\n              n_emb ** -0.5 for headi in range(n_head)]\n\nv = torch.stack([x_emb @ value for value in value_list], dim = -1).view\\\n                                        (xb.shape[0], block_size, -1)\n# ----------------------------------------------------------------------\n\nprint(v.shape)\nout = wei @ v\nout.shape\n\ntorch.Size([64, 8, 20])\n\n\ntorch.Size([64, 8, 20])\n\n\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\n# New code\n# -----------------------------------------------------------------\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size // n_head\n        \n        self.heads = nn.ModuleList([Head(self.head_size) \\\n                                    for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        \n        # Project the concatenated results\n        out = self.proj(out)\n        \n        return out\n# -----------------------------------------------------------------\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # New code\n        # -----------------------------------------------------\n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_head, head_size)\n        # -----------------------------------------------------\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n        \n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.size(1)))\n        x_emb = token_emb + position_emb\n        \n        # New code\n        # -----------------------------------------------------\n        # Multi-Head Attention\n        out = self.mul_head(self.ln1(x_emb))\n        # ----------------------------------------------------\n        \n        # Feed-Forward\n        out = self.ffwd(self.ln2(out))\n        \n        # Final layer normalization\n        out = self.ln_f(out)\n        \n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 13537\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2155, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n6. Residual\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size // n_head\n        \n        self.heads = nn.ModuleList([Head(self.head_size) \\\n                                    for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        \n        # Project the concatenated results\n        out = self.proj(out)\n        \n        return out\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_head, head_size)\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n        \n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.size(1)))\n        x_emb = token_emb + position_emb\n        \n        # New code\n        # ---------------------------------------------------------\n        # Multi-Head Attention\n        out = x_emb + self.mul_head(self.ln1(x_emb))\n        \n        # Feed-Forward\n        out = out + self.ffwd(self.ln2(out))\n        # ---------------------------------------------------------\n        \n        # Final layer normalization\n        out = self.ln_f(out)\n        \n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n    \nmodel = BigramLanguageModel()\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 13537\n\n\n\nmodel = BigramLanguageModel()\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2127, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "Appendix/transformer.html#clean-code",
    "href": "Appendix/transformer.html#clean-code",
    "title": "Transformer Architecture",
    "section": "Clean code",
    "text": "Clean code\n\nclass FeedFoward(nn.Module):\n    def __init__(self, n_emb):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(n_emb, 3 * n_emb),\n            nn.ReLU(),\n            nn.Linear(3 * n_emb, n_emb)\n        )\n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nclass Head(nn.Module):\n    def __init__(self, n_emb, block_size, head_size):\n        super().__init__()\n        \n        # Linear transformations for key, query, and value\n        self.key = nn.Linear(n_emb, head_size, bias=False)\n        self.query = nn.Linear(n_emb, head_size, bias=False)\n        self.value = nn.Linear(n_emb, head_size, bias=False)\n        \n        # Lower triangular matrix for masking\n        self.tril = torch.tril(torch.ones(block_size, block_size))\n        \n    def forward(self, x):\n        # Linear transformations for key and query\n        k = self.key(x)\n        q = self.query(x)\n        \n        # Compute the attention weights\n        wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n        \n        # Masking to make sure the network can't attend to the future positions\n        wei.masked_fill_(self.tril == 0, float('-inf'))\n        \n        # Applying softmax to get the attention probabilities\n        wei = F.softmax(wei, dim=-1)\n\n        # Linear transformation for value and computing the output\n        v = self.value(x)\n        out = wei @ v\n        \n        return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, n_emb, n_head, head_size):\n        super().__init__()\n        self.head_size = head_size\n        \n        self.heads = nn.ModuleList([Head(n_emb, block_size, head_size) for _ in range(n_head)])\n        self.proj = nn.Linear(self.head_size * n_head, n_emb)\n\n    def forward(self, x):\n        # Apply all attention heads in parallel\n        out = torch.cat([head(x) for head in self.heads], dim=-1)\n        \n        # Project the concatenated results\n        out = self.proj(out)\n        \n        return out\n\n\nclass Block(nn.Module):\n    def __init__(self, n_emb, n_head):\n        super().__init__()\n        \n        head_size = n_emb // n_head\n        \n        # Multi-Head Attention Layer\n        self.mul_head = MultiHeadAttention(n_emb, n_head, head_size)\n        \n        # Feed-Forward Layer\n        self.ffwd = FeedFoward(n_emb)\n        \n        # Layer Normalization Layers\n        self.ln1 = nn.LayerNorm(n_emb)\n        self.ln2 = nn.LayerNorm(n_emb)\n\n    def forward(self, x):\n        # Multi-Head Attention Block\n        x = x + self.mul_head(self.ln1(x))\n        \n        # Feed-Forward Block\n        x = x + self.ffwd(self.ln2(x))\n        \n        return x\n\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size, n_emb, block_size, n_head, n_layers):\n        super().__init__()\n        self.n_layers = n_layers\n        \n        # Embedding layers\n        self.C = nn.Embedding(vocab_size, n_emb)\n        self.position = nn.Embedding(block_size, n_emb)\n        \n        # Transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_emb, n_head) for _ in range(n_layers)])\n        \n        # Final layer normalization\n        self.ln_f = nn.LayerNorm(n_emb)\n        \n        # Linear layer for language modeling\n        self.lm_head = nn.Linear(n_emb, vocab_size)\n        \n    def forward(self, inp, targets):\n        # Embedding lookup for input data\n        token_emb = self.C(inp)\n        position_emb = self.position(torch.arange(inp.shape[1]))\n        x_emb = token_emb + position_emb\n        \n        # Transformer blocks\n        out = self.blocks(x_emb)\n        \n        # Final layer normalization\n        out = self.ln_f(out)\n\n        # Linear layer for language modeling\n        logits = self.lm_head(out)\n        \n        # Reshape logits for the cross-entropy loss\n        logits = logits.view(-1, logits.shape[-1])\n\n        # Compute the cross-entropy loss\n        loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n\nn_layers = 1\nmodel = BigramLanguageModel(vocab_size, n_emb, block_size, n_head, n_layers)\n\nnum_parameters = 0\nfor p in model.parameters():\n    num_parameters += p.numel()\nprint(\"Total number of trainable parameters:\", num_parameters)\n\nTotal number of trainable parameters: 15073\n\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    xb, yb = get_batch(data, batch_size, block_size)\n\n    logits, loss = model(xb, yb)\n\n    # Backward\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\nprint(loss)\n\ntensor(2.2123, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preface",
    "section": "",
    "text": "Dự án này đóng vai trò như một bài kiểm tra cuối khóa cho một môn học của tôi. Gần đây, những video hướng dẫn tuyệt vời từ anh Karpathy đã mở ra cho tôi cái nhìn rõ ràng hơn về Deep Learning và Large Language Model. Tuy nhiên, vì đang tự học, tôi không chắc chắn liệu mình thực sự hiểu sâu hay chỉ tự cho rằng mình đã hiểu. Điều này thúc đẩy tôi thực hiện dự án này với hy vọng nó không chỉ giúp người khác hiểu rõ hơn về LLAMA2, mà còn giúp tôi chứng minh rằng tôi đã thấu hiểu và có thể tiếp tục khám phá sâu hơn vào những kiến thức chuyên sâu hơn."
  },
  {
    "objectID": "index.html#the-goal",
    "href": "index.html#the-goal",
    "title": "Preface",
    "section": "",
    "text": "Dự án này đóng vai trò như một bài kiểm tra cuối khóa cho một môn học của tôi. Gần đây, những video hướng dẫn tuyệt vời từ anh Karpathy đã mở ra cho tôi cái nhìn rõ ràng hơn về Deep Learning và Large Language Model. Tuy nhiên, vì đang tự học, tôi không chắc chắn liệu mình thực sự hiểu sâu hay chỉ tự cho rằng mình đã hiểu. Điều này thúc đẩy tôi thực hiện dự án này với hy vọng nó không chỉ giúp người khác hiểu rõ hơn về LLAMA2, mà còn giúp tôi chứng minh rằng tôi đã thấu hiểu và có thể tiếp tục khám phá sâu hơn vào những kiến thức chuyên sâu hơn."
  },
  {
    "objectID": "index.html#why-llama2",
    "href": "index.html#why-llama2",
    "title": "Preface",
    "section": "Why Llama2",
    "text": "Why Llama2\nTôi quyết định lựa chọn Llama2 làm đề tài dự án của mình vì hiện tại nó đang phổ biến và được rất nhiều người quan tâm. Điều này cũng được thúc đẩy bởi vì anh Karpathy hiện đang trong quá trình thực hiện video bài giảng mới về LLAMA2. Do đó, việc chọn đề tài này có thể được coi như một cơ hội để tôi chuẩn bị trước, như việc tự chuẩn bị kiến thức trước khi đến lớp học của một giáo viên đầy nhiệt huyết."
  },
  {
    "objectID": "index.html#writing-style",
    "href": "index.html#writing-style",
    "title": "Preface",
    "section": "Writing Style",
    "text": "Writing Style\nTrong dự án này, mặc dù phần lớn sẽ được viết bằng tiếng Việt, tôi vẫn lựa chọn sử dụng tiếng Anh cho các thuật ngữ. Lý do là khi viết code, chúng ta thường sử dụng tiếng Anh cho tên biến và hàm, điều này giúp tăng tính nhất quán và dễ đọc của mã code. Ví dụ, từ “Weight” thường được viết tắt là “w” hoặc “weight” trong code, việc dịch sang “trọng số” có thể làm tăng độ phức tạp không cần thiết."
  },
  {
    "objectID": "initweight.html",
    "href": "initweight.html",
    "title": "Weight Initialization",
    "section": "",
    "text": "Trong chương này, chúng ta sẽ tìm hiểu về cách khởi tạo weight (weight initialization). Dường như cách khởi tạo weight mặc định của PyTorch không còn phù hợp nữa, và LLAMA2 đã thay đổi cách thức khởi tạo weight. Vì vậy, chúng ta sẽ đi sâu để hiểu tại sao điều này cần thiết và việc thay đổi khởi tạo weight này sẽ có tác dụng gì.\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport torch.nn.functional as F\nfrom einops import rearrange # einstein operation\nclass ModelArgs:\n    def __init__(self, vocab_size):\n        \n        self.max_sequence_len = 10\n        self.multiple_of = 5\n\n        self.rotary_dim = 3\n        self.n_layer = 4\n        self.batch_size = 16\n        self.n_head = 4\n        self.n_embd = 36\n        self.vocab_size = vocab_size\n\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(vocab_size)\nsample = 50\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer.pad_token = tokenizer.eos_token  \n\nsubset_dataset = dataset['train'][:sample]['text']\n\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=args.max_sequence_len  # Set the maximum sequence length\n)\n\ndata = tokenized_dataset['input_ids']\ndata.shape\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\ntorch.Size([50, 10])\nĐể việc thử nghiệm thuận tiện hơn, tôi đã giảm độ dài chuỗi (sequence length) xuống còn 10. Điều này sẽ giúp quá trình vẽ biểu đồ kết quả diễn ra nhanh chóng hơn.\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ncustom_dataset = CustomDataset(data)\ndata_loader = DataLoader(custom_dataset, batch_size=args.batch_size, shuffle=True)\n\ninput_ids = next(iter(data_loader))\n\n# xb = input_ids[:, :-1].contiguous()\n# yb = input_ids[:, 1:].contiguous()\n# xb.shape, yb.shape\ninput_ids.shape\n\ntorch.Size([16, 10])\nTừ đầu chương đến nay, chúng ta chỉ lấy dữ liệu một cách ngẫu nhiên. Bây giờ, tôi đã đưa dữ liệu vào data loader để có thể trích xuất dữ liệu theo từng batch, đồng thời vẫn giữ toàn bộ dữ liệu. Thay vì chia dữ liệu thành xb và yb, tôi giữ nguyên và gọi chúng là input_ids. Sau đó, ở giai đoạn tính loss, tôi sẽ tách logits ra thành x và y để tính toán loss."
  },
  {
    "objectID": "initweight.html#not-new",
    "href": "initweight.html#not-new",
    "title": "Weight Initialization",
    "section": "Not New",
    "text": "Not New\n\nEmbedding\n\nclass Embedding(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n        \n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n        \n        input_ids_embd = self.wte(input_ids)\n        \n        return input_ids_embd\n    \nembd = Embedding(args)\ninput_ids_embd = embd(input_ids)\ninput_ids_embd.shape\n\ntorch.Size([16, 10, 36])\n\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, args:ModelArgs, base = 10000):\n        super().__init__()\n        self.rotary_dim  = args.rotary_dim\n        \n        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        \n        self.cos_cache = None\n        self.sin_cache = None\n        \n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        \n        # Update cos sin cache\n        t = torch.arange(seqlen)\n        freqs = torch.outer(t, self.inv_freq)\n        \n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n        \n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n        \n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n        \n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n        \n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n        \n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n        \n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )\n\n\n\nRMS Norm\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n    \nattn_norm = RMSNorm(args.n_embd)\ninput_ids_embd_norm = attn_norm(input_ids_embd)\ninput_ids_embd_norm.shape\n\ntorch.Size([16, 10, 36])\n\n\n\n\nSelf Attention\n\nclass Attention(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.rotary_emb = RotaryEmbedding(args)\n        \n        self.head_dim = args.n_embd // args.n_head\n        opt_size = args.n_head * self.head_dim\n        hidden_size = args.n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n    def forward(self, input_ids_embd_norm):\n        qkv = self.Wqkv(input_ids_embd_norm)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n\n        # Rotary Query & Key\n        qkv = self.rotary_emb(qkv)\n        q, k, v = qkv.unbind(2)\n        \n        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n    \n# Normalize\nattn_norm = RMSNorm(args.n_embd)\nx_embd_norm = attn_norm(input_ids_embd)\n\nattn = Attention(args)\nattn_out = attn(input_ids_embd_norm)\n# add residual\nattn_out += input_ids_embd\nattn_out.shape\n\ntorch.Size([16, 10, 36])\n\n\n\n\nFeed Forward\n\nclass FeedForward(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        hidden_dim = 4 * args.n_embd\n        hidden_dim = int(2 * hidden_dim / 3)\n        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n        \n        self.w1 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, args.n_embd, bias=False)\n        self.w3 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n        \n        self.act = nn.SiLU()\n        \n    def forward(self, attn_out_norm):\n        hidden_states = self.w1(attn_out_norm) * self.w3(attn_out_norm)\n        hidden_states = self.act(hidden_states)\n        ffwd_out = self.w2(hidden_states)\n        \n        return ffwd_out\n    \n# Normalize\nffwd_norm = RMSNorm(args.n_embd)\nattn_out_norm = ffwd_norm(attn_out)\n\nffwd = FeedForward(args)\nffwd_out = ffwd(attn_out_norm)\n# add residual\nffwd_out += attn_out\nffwd_out.shape\n\ntorch.Size([16, 10, 36])\n\n\n\n\nTransformer Block\n\nclass TransfomerBlock(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.attention_norm = RMSNorm(args.n_embd)\n        self.ffwd_norm = RMSNorm(args.n_embd)\n        \n        self.attn = Attention(args)\n        self.ffwd = FeedForward(args)\n        \n    def forward(self, input_ids_embd):\n        \n        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n        \n        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n        \n        return ffwd_out\n    \nt_block = TransfomerBlock(args)\nffwd_out = t_block(input_ids_embd)\nffwd_out.shape\n\ntorch.Size([16, 10, 36])\n\n\n\n\nTransformer\n\nclass TransformerHead(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.norm = RMSNorm(args.n_embd)\n        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n        \n    def forward(self, ffwd_out):\n        h = self.norm(ffwd_out)\n        logits = self.linear(h)\n        \n        return logits\n    \nt_head = TransformerHead(args)\nlogits = t_head(ffwd_out)\nlogits.shape\n\ntorch.Size([16, 10, 50257])\n\n\n\nclass TransformerSequential(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n        modules.append(TransformerHead(args))\n        \n        self.layers = nn.Sequential(*modules)\n        \n    def forward(self, input_ids):\n        return self.layers(input_ids)\n    \nmodel = TransformerSequential(args)\nlogits = model(input_ids)\nlogits.shape\n\ntorch.Size([16, 10, 50257])\n\n\nTừ đầu đến nay gần như là tôi chỉ sao chép code từ chương trước sang thôi, không có gì mới để nói hết. Bây giờ chúng ta sẽ đi vào xem phân phối khởi tạo weight của model hiện tại để hiểu rõ hơn về cách mà weight được khởi tạo trong quá trình huấn luyện."
  },
  {
    "objectID": "initweight.html#experiment-init-weight",
    "href": "initweight.html#experiment-init-weight",
    "title": "Weight Initialization",
    "section": "Experiment Init Weight",
    "text": "Experiment Init Weight\n\nmodel_not = TransformerSequential(args)\n\nweight_names = {\n    \"Embedding\": 'layers.0.wte.weight',\n    \"Linear\": 'layers.1.attn.Wqkv.weight',\n}\n\nweight_dict_not = {}\nfor name, param in model_not.named_parameters():\n    param = param.view(-1).detach()\n    if name in weight_names.values():\n        param_name = [key for key, value in weight_names.items() if value == name][0]\n        weight_dict_not[param_name] = param\n        \nprint(f\"Embedding {args.n_embd} dims for each word in {args.vocab_size} words --&gt; 50257 * 36 = {50257 * 36}\")\nprint(\"Total Weight param of Embedding:\", len(weight_dict_not['Embedding']))\nprint(\"Total Weight param of Linear:   \", len(weight_dict_not['Linear']))\n\nEmbedding 36 dims for each word in 50257 words --&gt; 50257 * 36 = 1809252\nTotal Weight param of Embedding: 1809252\nTotal Weight param of Linear:    3888\n\n\nSố lượng tham số weight của embedding khá lớn lên đến 1 triệu 8, cũng hợp lý vì chúng ta hiện đang embedding 36 chiều cho mỗi từ trong vocab size có 50257 từ. Do đó, tổng số lượng tham số weight cho embedding là 36 * 50257. Tuy nhiên, có vẻ như PyTorch không sử dụng bias cho lớp embedding, vì tổng số biến tương đương với tổng số lượng weight.\n\nimport matplotlib.pyplot as plt\n\ndef show_graph_init_weight(weight_dict, title):\n    fig, axs = plt.subplots(1, len(weight_dict), figsize=(14, 6))\n\n    for i, (param_name, param_value) in enumerate(weight_dict.items()):\n        ax = axs[i]\n        ax.hist(param_value, bins='fd')\n        \n        ax.set_title(f\"{param_name}\\n \\\n            Mean: {param_value.mean():.5f}, Std: {param_value.std():.5f}\", fontsize=14)\n        \n        ax.set_xlabel(\"Value\", fontsize=10)\n        ax.set_ylabel(\"Frequency\", fontsize=10)\n\n    fig.suptitle(f\"{title} Apply Initialize Weight\", fontsize=16)\n    plt.tight_layout()\n    plt.show()\n    \nshow_graph_init_weight(weight_dict_not, \"Not\")\n\n\n\n\nCPU times: total: 42.1 s\nWall time: 45.3 s\n\n\nWeight trong lớp nn.Embedding được khởi tạo theo phân phối chuẩn (Gaussian) với mean (trung bình) bằng 0 và độ lệch chuẩn (std) bằng 1. Đối với weight trong lớp nn.Linear, chúng được khởi tạo theo phân phối đều (uniform distribution) với mean bằng 0 và độ lệch chuẩn bằng 0.095.\n\nclass TransformerSequential_Have(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        \n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n        modules.append(TransformerHead(args))\n        \n        self.layers = nn.Sequential(*modules)\n        \n        self.apply(self._init_weights)\n        \n    def forward(self, input_ids):\n        return self.layers(input_ids)\n    \n    # New code\n    # -----------------------------------------------------\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=.02)\n    # -----------------------------------------------------\n\n\nmodel_have = TransformerSequential_Have(args)\n\nweight_dict_have = {}\nfor name, param in model_have.named_parameters():\n    param = param.view(-1).detach()\n    if name in weight_names.values():\n        param_name = [key for key, value in weight_names.items() if value == name][0]\n        weight_dict_have[param_name] = param\n        \nprint(\"Total Weight param of Embedding:\", len(weight_dict_have['Embedding']))\nprint(\"Total Weight param of Linear:   \", len(weight_dict_have['Linear']))        \nshow_graph_init_weight(weight_dict_have, \"Have\")\n\nTotal Weight param of Embedding: 1809252\nTotal Weight param of Linear:    3888\nCPU times: total: 36.5 s\nWall time: 39.1 s\n\n\n\n\n\nLLAMA2 chọn cách khởi tạo weight trong các lớp Embedding và Linear bằng phân phối Gaussian với giá trị trung bình là 0 và độ lệch chuẩn là 0.02.\nChúng ta hãy cùng xem xét kết quả output tại các layer để hiểu rõ hơn về lý do họ chọn cách thức khởi tạo weight này.\n\ndef get_layers_output(model, input_ids, title):\n    layers_output = []\n    print(f\"{title} Apply Initialize Weight:\")\n    for layer in model.layers:\n        input_ids = layer(input_ids)\n        \n        layer_name = type(layer).__name__\n        print(f\"Output {layer_name}: {input_ids.shape}\")\n\n        layers_output.append((layer_name, input_ids.detach().numpy().flatten()))\n        \n    return layers_output\n\nlayers_output_not = get_layers_output(model_not, input_ids[0], 'Not')\nprint(\"\")\nlayers_output_have = get_layers_output(model_have, input_ids[0], 'Have')\n\nNot Apply Initialize Weight:\nOutput Embedding: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransformerHead: torch.Size([1, 10, 50257])\n\nHave Apply Initialize Weight:\nOutput Embedding: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransfomerBlock: torch.Size([1, 10, 36])\nOutput TransformerHead: torch.Size([1, 10, 50257])\n\n\n\nimport seaborn as sns\n\ndef show_density_plots(layers_output, ax, title):\n    for layer_name, layer_output in layers_output:\n        sns.kdeplot(layer_output, ax=ax, label=f\"Output {layer_name}\")\n        \n    ax.set_xlabel(\"Value\")\n    ax.set_ylabel(\"Density\")\n    ax.legend()\n    ax.set_title(title)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nshow_density_plots(layers_output_not, axes[0], \"Density Plots (Not)\")\nshow_density_plots(layers_output_have, axes[1], \"Density Plots (Have)\")\n\nplt.show()\n\n\n\n\nCPU times: total: 10.4 s\nWall time: 12.5 s\n\n\nCả hai cách khởi tạo trọng số đều tuân theo phân phối Gaussian, dẫn đến output của các layer có phân phối tương tự. Tuy nhiên, với cách tiếp cận mặc định của PyTorch, output của lớp Embedding và Transformer Block thường có mean và std gần bằng nhau, trước khi giảm mạnh khi tiến đến lớp Transformer Head (lớp cuối cùng). LLAMA2 đã thay đổi cách tiếp cận này, tạo ra một dạng phân phối với độ lệch chuẩn tăng dần từ các lớp Embedding và Transformer Block và tăng mạnh khi tiếp cận lớp cuối cùng.\n\ndef show_box_plots(layers_output, ax, title):\n    sns.boxplot(data=[data for _, data in layers_output], ax=ax, showfliers=False)\n        \n    ax.set_xlabel(\"Layer\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(title)\n    \n    x = range(len(layers_output))\n    ax.set_xticks(x)\n    \n    ax.set_xticklabels([layer_name for layer_name, _ in layers_output], rotation=45, ha='right')\n    \nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nshow_box_plots(layers_output_not, axes[0], \"Box Plots (Not)\")\nshow_box_plots(layers_output_have, axes[1], \"Box Plots (Have)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\ndef show_violin_plots(layers_output, ax, title):\n    sns.violinplot(data=[data for _, data in layers_output], ax=ax, inner='box', cut=0)\n    \n    ax.set_xlabel(\"Layer\")\n    ax.set_ylabel(\"Value\")\n    ax.set_title(title)        \n\n    x = range(len(layers_output))\n    ax.set_xticks(x)\n    ax.set_xticklabels([layer_name for layer_name, _ in layers_output], rotation=45, ha='right')\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\nshow_violin_plots(layers_output_not, axes[0], \"Violin Plots (Not)\")\nshow_violin_plots(layers_output_have, axes[1], \"Violin Plots (Have)\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "initweight.html#loss",
    "href": "initweight.html#loss",
    "title": "Weight Initialization",
    "section": "Loss",
    "text": "Loss\n\nclass TransformerLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels, shift_labels = True):\n        \n        # New code\n        # ----------------------------------------\n        if shift_labels:\n            logits = logits[..., :-1, :].contiguous()\n            labels = labels[..., 1:].contiguous()\n        # ----------------------------------------\n        \n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)                    \n                             \n        loss = self.loss_fct(logits, labels)\n\n        return loss\n    \nt_loss = TransformerLoss()\nloss = t_loss(logits, input_ids)\nloss\n\ntensor(11.1590, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nNhư đã đề cập trước đó, thay vì phải chia xb và yb ngay từ giai đoạn lấy dữ liệu, một cách tiếp cận khác là “shift labels” khi tính toán loss. Tôi chọn cách này vì trong code của LLAMA2, tôi thấy có một biến gọi là input_ids, cho thấy họ có thể đang áp dụng cách tiếp cận tương tự. Tuy nhiên, thực tế, bạn hoàn toàn có thể tách xb và yb ngay từ giai đoạn lấy dữ liệu nếu bạn cảm thấy phù hợp.\n\nn_epochs = 10\nlearning_rate = .001\n\nvocab_size = tokenizer.vocab_size\nargs = ModelArgs(vocab_size)\nmodel = TransformerSequential_Have(args)\nt_loss = TransformerLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\nlosses = []\nfor epochi in range(n_epochs):\n    for input_ids in data_loader:\n        logits = model(input_ids)\n        loss = t_loss(logits, input_ids)\n        losses.append(loss.item())\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \nplt.plot(losses)\nplt.title(\"Loss\");\n\n\n\n\nVậy là chúng ta đã hoàn thành tất cả các bước cần thiết để chuẩn bị cho việc huấn luyện mô hình. Trong chương tiếp theo, chúng ta sẽ tập trung vào việc cài đặt các siêu tham số (hyperparameters) mà LLAMA2 đã sử dụng để huấn luyện mô hình trên một tập dữ liệu nhỏ để tiến hành thử nghiệm. Điều này sẽ giúp chúng ta có cái nhìn cụ thể hơn về cách họ điều chỉnh các tham số để đạt được hiệu suất tốt nhất trong môi trường dữ liệu có hạn chế. Thêm vào đó, việc thực hiện thử nghiệm trên một tập dữ liệu nhỏ sẽ giúp chúng ta nhanh chóng đánh giá hiệu suất của mô hình trước khi triển khai trên toàn bộ tập dữ liệu."
  },
  {
    "objectID": "hyperparameter.html",
    "href": "hyperparameter.html",
    "title": "Hyperparameter",
    "section": "",
    "text": "Chúng ta đã hoàn thành xong việc triển khai kiến trúc transformer của LLAMA2, tuy nhiên cho đến nay chúng ta chỉ chọn các giá trị cho các tham số trong Model Args tương đối nhỏ. Bây giờ chúng ta sẽ cùng nhau tìm hiểu và chọn các giá trị cụ thể mà LLAMA2 đã sử dụng trong quá trình huấn luyện.\n\npip -q install transformers datasets einops pytorch_lightning wandb\n\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom einops import rearrange # einstein operation\n\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\nBây giờ chúng ta đã thực sự vào train model rồi nên cũng đã đến lúc sử dụng tokenizer của LLAMA2. Để sử dụng tokenizer này, chúng ta cần đăng nhập vào tài khoản Hugging Face và đồng ý với các điều khoản của LLAMA2.\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\nprint(\"Vocab Size When Use Tokenizer of LLAMA2:\", tokenizer.vocab_size)\n\nVocab Size When Use Tokenizer of LLAMA2: 32000\n\n\nSử dụng tokenizer của LLAMA2 thì vocab size của chúng ta sẽ là 32000.\n\nclass ModelArgs:\n    def __init__(self):\n\n        self.norm_eps = 1e-5\n\n        self.initializer_range = 0.02\n        self.learning_rate = None # experiment below\n        self.n_epochs = 100\n\n        self.rotary_dim = 128 #(self.n_embd // self.n_head) / 2\n\n        # divided by 8\n        self.n_head = 4 #32\n        self.n_embd = 512 #4096\n        self.max_sequence_len = 256 #2048\n        self.multiple_of = 32 #256\n        \n        self.n_layer = 4 #32\n\n        self.batch_size = 32 #32\n        self.vocab_size = 32000\n\nargs = ModelArgs()\n\nSau khi thực hiện thử nghiệm và gặp liên tục vấn đề “out of memory”, tôi đã quyết định giảm xuống 1/8 so với các giá trị gốc được sử dụng trong LLAMA2 đối với các biến ảnh hưởng đến tổng số biến. Do đó, chúng ta chỉ cần tập trung vào việc thử nghiệm với learning rate để tìm ra giá trị phù hợp nhất.\nĐối với biến rotary dim, tôi đã lấy từ Microsoft phiên bản 1.5. Tôi nhận thấy rằng họ chỉ xoay một nửa so với head size, tức là bằng (n_embd // n_head) // 2.\n\nsample_train   = 960\nsample_val = int(.1 * sample_train)\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer.pad_token = tokenizer.eos_token\n\nsubset_trainset = dataset['train'][:sample_train]['text']\nsubset_valset = dataset['validation'][:sample_val]['text']\n\ntokenized_trainset = tokenizer(\n    subset_trainset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=args.max_sequence_len  # Set the maximum sequence length\n)\n\ntokenized_valset = tokenizer(\n    subset_valset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=args.max_sequence_len  # Set the maximum sequence length\n)\n\ntrain_data = tokenized_trainset['input_ids']\nval_data = tokenized_valset['input_ids']\ntrain_data.shape, val_data.shape\n\nRepo card metadata block was not found. Setting CardData to empty.\nWARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n\n\n(torch.Size([960, 256]), torch.Size([96, 256]))\n\n\nĐã đến giai đoạn train model nên ta sẽ sử dụng dữ liệu train set và valid set.\n\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\nimport multiprocessing\n\ncpu_count = multiprocessing.cpu_count()\nprint(f\"Number of CPU cores: {cpu_count}\")\n\ncustom_trainset = CustomDataset(train_data)\ntrain_loader = DataLoader(custom_trainset, batch_size=args.batch_size, shuffle=True, num_workers=cpu_count)\n\ncustom_valset = CustomDataset(val_data)\nval_loader = DataLoader(custom_valset, batch_size=args.batch_size, num_workers=cpu_count)\n\n# input_ids = next(iter(train_loader))\n# input_ids.shape\n\nNumber of CPU cores: 2\n\n\nData Loader hỗ trợ việc sử dụng nhiều core CPU có sẵn trên máy tính, nên tôi đã kiểm tra và sử dụng số lượng core CPU hiện có trên máy tính để trong quá trình xử lý dữ liệu nó sẽ giúp tối ưu hóa hiệu suất của Data Loader.\n\nclass Embedding(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n\n        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n\n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n\n        input_ids_embd = self.wte(input_ids)\n\n        return input_ids_embd\n\n# embd = Embedding(args)\n# input_ids_embd = embd(input_ids)\n# input_ids_embd.shape\n\nĐể giảm thiểu việc sử dụng bộ nhớ, tôi đã quyết định không hiển thị chi tiết output của từng class trong quá trình thử nghiệm. Thay vào đó, tôi đã chuyển các dòng code thể hiện output của từng class thành dạng comment để thể hiện rằng không có bất kỳ thay đổi nào so với chương trước.\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, args:ModelArgs, base = 10000):\n        super().__init__()\n        self.rotary_dim = args.rotary_dim\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self.cos_cache = None\n        self.sin_cache = None\n\n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n\n        # Update cos sin cache\n        t = torch.arange(seqlen, device = qkv.device)\n        freqs = torch.outer(t, self.inv_freq)\n\n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n\n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n\n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n\n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n\n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n\n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n\n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n# attn_norm = RMSNorm(args.n_embd)\n# input_ids_embd_norm = attn_norm(input_ids_embd)\n# input_ids_embd_norm.shape\n\n\nclass Attention(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n\n        self.rotary_emb = RotaryEmbedding(args)\n\n        self.head_dim = args.n_embd // args.n_head\n        opt_size = args.n_head * self.head_dim\n        hidden_size = args.n_embd\n\n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n\n    def forward(self, input_ids_embd_norm):\n        qkv = self.Wqkv(input_ids_embd_norm)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n\n        # Rotary Query & Key\n        qkv = self.rotary_emb(qkv)\n        q, k, v = qkv.unbind(2)\n\n        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n\n        attn_out = self.out_proj(output)\n\n        return attn_out\n\n# # Normalize\n# attn_norm = RMSNorm(args.n_embd)\n# x_embd_norm = attn_norm(input_ids_embd)\n\n# attn = Attention(args)\n# attn_out = attn(input_ids_embd_norm)\n# # add residual\n# attn_out += input_ids_embd\n# attn_out.shape\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        hidden_dim = 4 * args.n_embd\n        hidden_dim = int(2 * hidden_dim / 3)\n        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n\n        self.w1 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, args.n_embd, bias=False)\n        self.w3 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n\n        self.act = nn.SiLU()\n\n    def forward(self, attn_out_norm):\n        hidden_states = self.w1(attn_out_norm) * self.w3(attn_out_norm)\n        hidden_states = self.act(hidden_states)\n        ffwd_out = self.w2(hidden_states)\n\n        return ffwd_out\n\n# # Normalize\n# ffwd_norm = RMSNorm(args.n_embd)\n# attn_out_norm = ffwd_norm(attn_out)\n\n# ffwd = FeedForward(args)\n# ffwd_out = ffwd(attn_out_norm)\n# # add residual\n# ffwd_out += attn_out\n# ffwd_out.shape\n\n\nclass TransfomerBlock(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n\n        self.attention_norm = RMSNorm(args.n_embd, args.norm_eps)\n        self.ffwd_norm = RMSNorm(args.n_embd, args.norm_eps)\n\n        self.attn = Attention(args)\n        self.ffwd = FeedForward(args)\n\n    def forward(self, input_ids_embd):\n\n        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n\n        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n\n        return ffwd_out\n\n# t_block = TransfomerBlock(args)\n# ffwd_out = t_block(input_ids_embd)\n# ffwd_out.shape\n\n\nclass TransformerHead(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n\n        self.norm = RMSNorm(args.n_embd, args.norm_eps)\n        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n\n    def forward(self, ffwd_out):\n        h = self.norm(ffwd_out)\n        logits = self.linear(h)\n\n        return logits\n\n# t_head = TransformerHead(args)\n# logits = t_head(ffwd_out)\n# logits.shape\n\n\nclass TransformerSequential(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.initializer_range = args.initializer_range\n\n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n        modules.append(TransformerHead(args))\n\n        self.layers = nn.Sequential(*modules)\n        self.apply(self._init_weights)\n\n    def forward(self, input_ids):\n        return self.layers(input_ids)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n\n# model = TransformerSequential(args)\n# logits = model(input_ids)\n# logits.shape\n\n\nclass TransformerLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels, shift_labels = True):\n        if shift_labels:\n            logits = logits[..., :-1, :].contiguous()\n            labels = labels[..., 1:].contiguous()\n\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)\n\n        loss = self.loss_fct(logits, labels)\n\n        return loss\n\n# t_loss = TransformerLoss()\n# loss = t_loss(logits, input_ids)\n# loss\n\n\nimport pytorch_lightning as pl\nimport wandb\n\nclass ModelForVisualization(pl.LightningModule):\n    def __init__(self, args, lr):\n        super().__init__()\n        self.learning_rate = lr\n\n        self.model = TransformerSequential(args)\n        self.t_loss = TransformerLoss()\n\n    def forward(self, input_ids):\n        return self.model(input_ids)\n\n    def training_step(self, batch):\n        input_ids = batch\n        logits = self(input_ids)\n        loss = self.t_loss(logits, input_ids)\n\n        wandb.log({\"train loss\": loss})\n\n        return loss\n\n    def validation_step(self, batch):\n        input_ids = batch\n        logits = self(input_ids)\n        loss = self.t_loss(logits, input_ids)\n\n        wandb.log({\"valid loss\": loss})\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.learning_rate,\n        )\n\n        return optimizer\n\nTôi đã sử dụng thư viện pytorch_lightning vì nó giúp tạo thanh tiến trình (progress bar) để hiển thị trạng thái của mô hình đang huấn luyện. Ngoài ra, tôi cũng sử dụng weight and bias (wanb) để theo dõi biểu đồ loss trực tuyến, giúp quá trình huấn luyện trở nên thuận tiện hơn.\nDưới đây là đoạn code mà tôi sử dụng để thử nghiệm tốc độ học (learning rate). Vì quá trình này mất rất nhiều thời gian, tôi đã chạy nó trong hơn 8 giờ nên tôi để nó dưới dạng comment và chỉ hiển thị code của learning rate tốt nhất mà tôi đã chọn. Đây là cái nhìn tổng quan về quá trình này.\n”\nVà tới đây là kết thúc chương này. Trong chương tiếp theo, tôi sẽ tiến hành huấn luyện mô hình với một lượng lớn dữ liệu và clean code một chút.\n\nwandb.login()\n\nargs = ModelArgs()\n\n# learning_rates_to_try = [0.1, 0.01, 0.001, 0.0005]\n# learning_rates_to_try = np.linspace(0.001, 0.0001, 10)\n\n# for lr in learning_rates_to_try:\n#     name = f\"run_lr_{lr}\"\n#     wandb.init(project=\"llama2\", config=args, name=name)\n#     model = ModelForVisualization(args, lr)\n\n#     trainer = pl.Trainer(max_epochs=args.n_epochs)\n#     trainer.fit(model, train_loader, val_loader)\n\nbest_learning_rate = 0.0003\nmodel = ModelForVisualization(args, best_learning_rate)\ntrainer = pl.Trainer(max_epochs=6)\ntrainer.fit(model, train_loader, val_loader)\n\n\n\n\nwandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\nwandb: You can find your API key in your browser here: https://wandb.ai/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nwandb: Currently logged in as: nvtai0452. Use `wandb login --relogin` to force relogin\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nWARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\n/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\nINFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_121416-zt6vny14\n\n\nSyncing run run_lr_0.001 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/zt6vny14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:zt6vny14) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▇▆▆▆▆▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂▂▂▂▁▁▁▂▂▂▂▁\n\n\nvalid loss\n▃▁▁▁▁▂▂▂▃▄▄▄▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.36344\n\n\nvalid loss\n3.93152\n\n\n\n\n\n\n\n View run run_lr_0.001 at: https://wandb.ai/nvtai0452/llama2/runs/zt6vny14Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_121416-zt6vny14/logs\n\n\nSuccessfully finished last run (ID:zt6vny14). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_125142-snqu74s0\n\n\nSyncing run run_lr_0.0009 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/snqu74s0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:snqu74s0) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▇▆▅▅▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▂▂▂▁▁▁▁▂▂▂▁\n\n\nvalid loss\n▃▂▁▁▁▂▂▂▃▄▄▄▄▅▆▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇██\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.20064\n\n\nvalid loss\n3.94179\n\n\n\n\n\n\n\n View run run_lr_0.0009 at: https://wandb.ai/nvtai0452/llama2/runs/snqu74s0Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_125142-snqu74s0/logs\n\n\nSuccessfully finished last run (ID:snqu74s0). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_132938-rvx2dzd5\n\n\nSyncing run run_lr_0.0008 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/rvx2dzd5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:rvx2dzd5) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▇▆▆▅▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▂▂▂▁▁▁▂▁▂▂▁\n\n\nvalid loss\n▃▂▁▁▁▁▂▂▃▄▄▅▅▅▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇█▇██▇█████\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.32851\n\n\nvalid loss\n3.9453\n\n\n\n\n\n\n\n View run run_lr_0.0008 at: https://wandb.ai/nvtai0452/llama2/runs/rvx2dzd5Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_132938-rvx2dzd5/logs\n\n\nSuccessfully finished last run (ID:rvx2dzd5). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_140745-21qmtq46\n\n\nSyncing run run_lr_0.0007000000000000001 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/21qmtq46\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:21qmtq46) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▇▆▅▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▁▁▁▂▂▂▁▁▁▁▂▁▂▁\n\n\nvalid loss\n▃▂▁▁▁▁▂▂▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇█▇██▇██\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.20635\n\n\nvalid loss\n3.98119\n\n\n\n\n\n\n\n View run run_lr_0.0007000000000000001 at: https://wandb.ai/nvtai0452/llama2/runs/21qmtq46Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_140745-21qmtq46/logs\n\n\nSuccessfully finished last run (ID:21qmtq46). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_144551-mate1gtw\n\n\nSyncing run run_lr_0.0006000000000000001 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/mate1gtw\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:mate1gtw) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▆▅▅▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▂▂▂▁\n\n\nvalid loss\n▄▂▁▁▁▁▂▂▃▄▄▄▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇█▇███████▇▇█\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.29609\n\n\nvalid loss\n3.83442\n\n\n\n\n\n\n\n View run run_lr_0.0006000000000000001 at: https://wandb.ai/nvtai0452/llama2/runs/mate1gtwSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_144551-mate1gtw/logs\n\n\nSuccessfully finished last run (ID:mate1gtw). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_152401-ace73pkp\n\n\nSyncing run run_lr_0.0005 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/ace73pkp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:ace73pkp) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂▂▂▂▂▁▁▂▁▁▂▂▁\n\n\nvalid loss\n▄▂▁▁▁▁▂▂▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▆▇▇▆▇▇▇▇▇▇█▇██▇██\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.31785\n\n\nvalid loss\n3.92018\n\n\n\n\n\n\n\n View run run_lr_0.0005 at: https://wandb.ai/nvtai0452/llama2/runs/ace73pkpSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_152401-ace73pkp/logs\n\n\nSuccessfully finished last run (ID:ace73pkp). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_160227-lbe3t0fz\n\n\nSyncing run run_lr_0.0004000000000000001 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/lbe3t0fz\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:lbe3t0fz) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▆▅▅▅▅▅▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁\n\n\nvalid loss\n▅▂▁▁▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇█▇█████▇▇█\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.31858\n\n\nvalid loss\n3.84997\n\n\n\n\n\n\n\n View run run_lr_0.0004000000000000001 at: https://wandb.ai/nvtai0452/llama2/runs/lbe3t0fzSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_160227-lbe3t0fz/logs\n\n\nSuccessfully finished last run (ID:lbe3t0fz). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_164059-kd22o4kb\n\n\nSyncing run run_lr_0.00030000000000000014 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/kd22o4kb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinishing last run (ID:kd22o4kb) before initializing another...\n\n\nWaiting for W&B process to finish... (success).\n\n\n\n\n\n\n\nRun history:\n\n\n\ntrain loss\n█▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nvalid loss\n▆▃▁▁▁▁▂▂▂▃▃▃▃▄▅▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇█▇██\n\n\n\n\n\n\nRun summary:\n\n\n\n\n\ntrain loss\n1.25272\n\n\nvalid loss\n3.97291\n\n\n\n\n\n\n\n View run run_lr_0.00030000000000000014 at: https://wandb.ai/nvtai0452/llama2/runs/kd22o4kbSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20231017_164059-kd22o4kb/logs\n\n\nSuccessfully finished last run (ID:kd22o4kb). Initializing new run:\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231017_171928-tpz23mt0\n\n\nSyncing run run_lr_0.0002000000000000001 to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/tpz23mt0"
  },
  {
    "objectID": "trainmodel.html",
    "href": "trainmodel.html",
    "title": "Train Model",
    "section": "",
    "text": "”\nTrong chương này, do hạn chế về tài nguyên, chúng ta chỉ có thể huấn luyện trên một phần nhỏ của tập dữ liệu, với khoảng 165,000 câu thay vì toàn bộ. Sau gần 6 giờ huấn luyện, tôi vẫn chưa chắc chắn liệu mình đang làm đúng hay không vì model vẫn chỉ mắc kẹt ở giá trị loss là 2.7. Tuy nhiên, do anh Karpathy đang trong quá trình thực hiện video giảng về LLAMA2, vì vậy tôi quyết định tạm dừng dự án tại đây. Tôi sẽ quay trở lại cải thiện model và dự án của mình sau khi xem video về llama2 của anh Karpathy được đăng tải.\n\npip install transformers datasets einops pytorch_lightning wandb\n\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport multiprocessing\nimport pytorch_lightning as pl\nimport wandb\nfrom einops import rearrange # einstein operation\n\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\n\n\n\nclass ModelArgs:\n    def __init__(self):\n\n        self.check_point_per_batch = 2000\n        self.learning_rate = 0.0003\n        self.n_epochs = 30\n\n        self.n_head = 4 #32\n        self.n_embd = 512 #4096\n        self.max_sequence_len = 256 #2048\n        self.multiple_of = 32 #256\n        self.rotary_dim = 64 # head_dim // 2 (head_dim = n_embd // n_head)\n        self.n_layer = 4 #32\n        self.batch_size = 32 #32\n        self.vocab_size = 32000\n        self.norm_eps = 1e-5\n        self.initializer_range = 0.02\n\nargs = ModelArgs()\n\n\nsample_train = 150_000\nsample_val = int(.1 * sample_train)\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer.pad_token = tokenizer.eos_token\n\nsubset_trainset = dataset['train'][:sample_train]['text']\nsubset_valset = dataset['validation'][:sample_val]['text']\n\ntokenized_trainset = tokenizer(\n    subset_trainset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=args.max_sequence_len  # Set the maximum sequence length\n)\n\ntokenized_valset = tokenizer(\n    subset_valset,\n    return_tensors='pt',\n    padding='max_length',  # Pad sequences to the max_seq_length\n    truncation=True,  # Truncate sequences if they exceed max_seq_length\n    max_length=args.max_sequence_len  # Set the maximum sequence length\n)\n\nRepo card metadata block was not found. Setting CardData to empty.\nWARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ncustom_trainset = CustomDataset(tokenized_trainset['input_ids'])\ncustom_valset = CustomDataset(tokenized_valset['input_ids'])\n\ncpu_count = multiprocessing.cpu_count()\ntrain_loader = DataLoader(custom_trainset, batch_size=args.batch_size, shuffle=True, num_workers=cpu_count)\nval_loader = DataLoader(custom_valset, batch_size=args.batch_size, num_workers=cpu_count)\n\n\nclass Embedding(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n\n    def forward(self, input_ids):\n        input_shape = input_ids.shape[-1]\n        input_ids = input_ids.view(-1, input_shape)\n\n        hidden_states = self.wte(input_ids)\n\n        return hidden_states\n\n\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, args:ModelArgs, base = 10000):\n        super().__init__()\n        self.rotary_dim = args.rotary_dim\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        self.cos_cache = None\n        self.sin_cache = None\n\n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n\n        # Update cos sin cache\n        t = torch.arange(seqlen, device = qkv.device)\n        freqs = torch.outer(t, self.inv_freq)\n\n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n\n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n\n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n\n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n\n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n\n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n\n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )\n\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n\n\nclass Attention(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        self.head_dim = args.n_embd // args.n_head\n        opt_size = args.n_head * self.head_dim\n        hidden_size = args.n_embd\n\n        self.rotary_emb = RotaryEmbedding(args)\n\n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n\n    def forward(self, hidden_states):\n        qkv = self.Wqkv(hidden_states)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n\n        # Rotary Query & Key\n        qkv = self.rotary_emb(qkv)\n        q, k, v = qkv.unbind(2)\n\n        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n\n        attn_out = self.out_proj(output)\n\n        return attn_out\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        hidden_dim = 4 * args.n_embd\n        hidden_dim = int(2 * hidden_dim / 3)\n        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)\n\n        self.w1 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, args.n_embd, bias=False)\n        self.w3 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n\n        self.act = nn.SiLU()\n\n    def forward(self, hidden_states):\n        hidden_states = self.w1(hidden_states) * self.w3(hidden_states)\n        hidden_states = self.act(hidden_states)\n        ffwd_out = self.w2(hidden_states)\n\n        return ffwd_out\n\n\nclass TransfomerBlock(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n\n        self.attention_norm = RMSNorm(args.n_embd, args.norm_eps)\n        self.ffwd_norm = RMSNorm(args.n_embd, args.norm_eps)\n\n        self.attn = Attention(args)\n        self.ffwd = FeedForward(args)\n\n    def forward(self, input_ids_embd):\n\n        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n\n        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n\n        return ffwd_out\n\n\nclass TransformerHead(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n\n        self.norm = RMSNorm(args.n_embd, args.norm_eps)\n        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n\n    def forward(self, hidden_states):\n        hidden_states = self.norm(hidden_states)\n        logits = self.linear(hidden_states)\n\n        return logits\n\n\nclass TransformerSequential(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.initializer_range = args.initializer_range\n\n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n        modules.append(TransformerHead(args))\n\n        self.layers = nn.Sequential(*modules)\n        self.apply(self._init_weights)\n\n    def forward(self, input_ids):\n        return self.layers(input_ids)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.initializer_range)\n\n\nclass TransformerLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels, shift_labels = True):\n        if shift_labels:\n            logits = logits[..., :-1, :].contiguous()\n            labels = labels[..., 1:].contiguous()\n\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)\n\n        loss = self.loss_fct(logits, labels)\n\n        return loss\n\n\nclass ModelForVisualization(pl.LightningModule):\n    def __init__(self, args):\n        super().__init__()\n        self.learning_rate = args.learning_rate\n        self.ck_point_per_batch = args.check_point_per_batch\n\n        self.model = TransformerSequential(args)\n        self.t_loss = TransformerLoss()\n\n    def forward(self, input_ids):\n        return self.model(input_ids)\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch\n        logits = self(input_ids)\n        loss = self.t_loss(logits, input_ids)\n\n        wandb.log({\"train loss\": loss})\n\n        # Checkpoint to W&B\n        if batch_idx % self.ck_point_per_batch == 0:\n            self.save_checkpoint(f\"checkpoint_{batch_idx}.ckpt\")\n\n        return loss\n\n    def validation_step(self, batch):\n        input_ids = batch\n        logits = self(input_ids)\n        loss = self.t_loss(logits, input_ids)\n\n        wandb.log({\"valid loss\": loss})\n\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(\n            self.model.parameters(),\n            lr=self.learning_rate,\n        )\n\n        return optimizer\n\n    def save_checkpoint(self, filename):\n        checkpoint = {'state_dict': self.state_dict()}\n        torch.save(checkpoint, filename)\n        wandb.save(filename)\n\n\nwandb.login()\n\nargs = ModelArgs()\n\nname = \"train model\"\nwandb.init(project=\"llama2\", config=args, name=name)\nmodel = ModelForVisualization(args)\n\ntrainer = pl.Trainer(max_epochs=args.n_epochs)\ntrainer.fit(model, train_loader, val_loader)\n\nwandb.finish()\n\n\n\n\nwandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\nwandb: You can find your API key in your browser here: https://wandb.ai/authorize\nwandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\nwandb: Currently logged in as: nvtai0452. Use `wandb login --relogin` to force relogin\nINFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\nINFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\nWARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO:pytorch_lightning.callbacks.model_summary:\n  | Name   | Type                  | Params\n-------------------------------------------------\n0 | model  | TransformerSequential | 45.5 M\n1 | t_loss | TransformerLoss       | 0     \n-------------------------------------------------\n45.5 M    Trainable params\n0         Non-trainable params\n45.5 M    Total params\n181.845   Total estimated model params size (MB)\n\n\n ··········\n\n\nTracking run with wandb version 0.15.12\n\n\nRun data is saved locally in /content/wandb/run-20231018_121605-42l26vl1\n\n\nSyncing run train model to Weights & Biases (docs)\n\n\n View project at https://wandb.ai/nvtai0452/llama2\n\n\n View run at https://wandb.ai/nvtai0452/llama2/runs/42l26vl1"
  },
  {
    "objectID": "llamap1.html",
    "href": "llamap1.html",
    "title": "Llama2 Architecture (P1)",
    "section": "",
    "text": "”\nTrong kiến trúc transformers của LLAMA2, có tổng cộng 4 điểm khác biệt chính so với kiến trúc mà chúng ta đã học trước đó. Tuy nhiên, trong chương này, chúng ta sẽ chỉ nói về Rotary Positional Encodings vì có thể chỉ tốn 5 phút nếu bạn chỉ muốn nắm ý tưởng chính của nó, hoặc có thể mất hàng giờ nếu bạn muốn thực sự tìm hiểu sâu hơn về nó.\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nfrom einops import rearrange # einstein operation\nsample = 20\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\n\nsubset_dataset = dataset['train'][:sample]['text']\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\n\ndata = tokenized_dataset['input_ids']\ndata.shape\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\ntorch.Size([20, 219])\nclass ModelArgs:\n    def __init__(self, sequence_len, vocab_size):\n        \n        self.n_layer = 2\n        \n        self.batch_size = 16\n        self.n_head = 4\n        self.n_embd = 36\n        self.sequence_len = sequence_len\n        self.vocab_size = vocab_size\n\n\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(sequence_len, vocab_size)\ndef get_batch(data, batch_size):\n    idx = torch.randint(0, len(data), size=(batch_size,))\n    batch = data[idx]\n\n    xb = batch[:, :-1].contiguous()\n    yb = batch[:, 1:].contiguous()\n    \n    return xb, yb\n\nxb, yb = get_batch(data, args.batch_size)\nxb.shape, yb.shape\n\n(torch.Size([16, 218]), torch.Size([16, 218]))"
  },
  {
    "objectID": "llamap1.html#embedding",
    "href": "llamap1.html#embedding",
    "title": "Llama2 Architecture (P1)",
    "section": "Embedding",
    "text": "Embedding\n”\n\nclass Embedding(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n        \n    def forward(self, input_ids):\n        input_ids_embd = self.wte(input_ids)\n        \n        return input_ids_embd\n    \nembd = Embedding(args)\nx_embd = embd(xb)\nx_embd.shape\n\ntorch.Size([16, 218, 36])\n\n\nTrong phần Embedding, thay vì sử dụng phần Position Embedding như trước, LLAMA2 sử dụng phần “rotary position embedding” mà chúng ta sẽ đề cập ở phần dưới."
  },
  {
    "objectID": "llamap1.html#rotary-position-embedding",
    "href": "llamap1.html#rotary-position-embedding",
    "title": "Llama2 Architecture (P1)",
    "section": "Rotary Position Embedding",
    "text": "Rotary Position Embedding\n”\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, args:ModelArgs, base = 10000):\n        super().__init__()\n        self.rotary_dim  = 3\n        \n        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        \n        self.cos_cache = None\n        self.sin_cache = None\n        \n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        \n        # Update cos sin cache\n        t = torch.arange(seqlen)\n        freqs = torch.outer(t, self.inv_freq)\n        \n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n        \n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n        \n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n        \n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n        \n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n        \n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n        \n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )\n\nRotary Position Embedding, là một phiên bản tối ưu hóa của việc Position Embedding thông thường. Thay vì đơn giản là thêm một vector vị trí cho từng từ trong câu, chúng ta thực hiện một phép xoay (rotation) trên các giá trị trong ma trận Query và ma trận Key.\nMục tiêu chính của Rotary Position Embedding là tối ưu hóa tốc độ tính toán. Bằng cách áp dụng phép xoay này, chúng ta giúp mô hình học cách tương tác vị trí một cách hiệu quả hơn và giảm sự phức tạp của tính toán so với việc sử dụng Position Embedding truyền thống bằng vector vị trí.\nĐoạn code trên được dựa trên code của Microsoft Phi-1.5. Tôi thấy đoạn code này gọn gàng và dễ hiểu hơn, vì vậy tôi đã sử dụng nó. Tuy nhiên, về bản chất, không có sự khác biệt lớn so với code của LLAMA2.\nTrong chương này chúng ta sẽ tạm dừng tại đây. Tôi sẽ chỉ thêm một dòng code mới để thực hiện phép xoay QKV trong class Attention. Còn lại, tôi sẽ giữ nguyên như chương trước. Lý do là vì tôi nghĩ sẽ có nhiều bạn muốn thực sự hiểu toàn bộ mã code ở trên và sẽ cần dành nhiều thời gian để nghiên cứu. Và nếu như vậy, chương này đã là quá dài và nên tạm dừng để họ có thể giải lao và chuyển sang một chương mới vào ngày hôm sau. Tuy nhiên nếu bạn giống tôi, sẵn sàng tiếp tục học kiến thức mới sau khi đã hiểu ý tưởng cơ bản của Rotary Position Embedding, hãy cứ thoải mái mà chuyển sang chương tiếp theo.\n\nclass Attention(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.rotary_emb = RotaryPositionEmbedding(args)\n        \n        self.head_dim = args.n_embd // args.n_head\n        opt_size = args.n_head * self.head_dim\n        hidden_size = args.n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n    def forward(self, input_ids_embd_norm):\n        seq_len = input_ids_embd_norm.shape[1]\n        \n        qkv = self.Wqkv(input_ids_embd_norm)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n        \n        # New code\n        # Rotary Query & Key\n        # -------------------------\n        qkv = self.rotary_emb(qkv)\n        # -------------------------\n        \n        q, k, v = qkv.unbind(2)\n        \n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n        \n        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n        scores += mask\n        \n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention_weights, v)\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n\n\n# Normalize\nattn_norm = nn.LayerNorm(args.n_embd)\nx_embd_norm = attn_norm(x_embd)\n\nattn = Attention(args)\nattn_out = attn(x_embd_norm)\n# add residual\nattn_out += x_embd\nattn_out.shape\n\ntorch.Size([16, 218, 36])\n\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        hidden_size = 4 * args.n_embd\n        \n        self.fc1 = nn.Linear(args.n_embd, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, args.n_embd)\n        self.act = nn.ReLU()\n        \n    def forward(self, attn_out_norm):\n        hidden_states = self.fc1(attn_out_norm)\n        hidden_states = self.act(hidden_states)\n        ffwd_out = self.fc2(hidden_states)\n        \n        return ffwd_out\n\n\n# Normalize\nffwd_norm = nn.LayerNorm(args.n_embd)\nattn_out_norm = ffwd_norm(attn_out)\n\nffwd = FeedForward(args)\nffwd_out = ffwd(attn_out_norm)\n# add residual\nffwd_out += attn_out\nffwd_out.shape\n\ntorch.Size([16, 218, 36])\n\n\n\nclass TransfomerBlock(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.attention_norm = nn.LayerNorm(args.n_embd)\n        self.ffwd_norm = nn.LayerNorm(args.n_embd)\n        \n        self.attn = Attention(args)\n        self.ffwd = FeedForward(args)\n        \n    def forward(self, input_embd):\n        \n        attn_out = input_embd + self.attn(self.attention_norm(input_embd))\n        \n        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n        \n        return ffwd_out\n\n\nt_block = TransfomerBlock(args)\nffwd_out = t_block(x_embd)\nffwd_out.shape\n\ntorch.Size([16, 218, 36])\n\n\n\nclass TransformerHead(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.norm = nn.LayerNorm(args.n_embd)\n        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n        \n    def forward(self, ffwd_out):\n        ffwd_out_norm = self.norm(ffwd_out)\n        logits = self.linear(ffwd_out_norm)\n        \n        return logits\n    \nt_head = TransformerHead(args)\nlogits = t_head(ffwd_out)\nlogits.shape\n\ntorch.Size([16, 218, 50257])\n\n\n\nclass TransformerSequential(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n        modules.append(TransformerHead(args))\n        \n        self.layers = nn.Sequential(*modules)\n        \n    def forward(self, input_ids):\n        logits = self.layers(input_ids)\n        \n        return logits\n\n\nmodel = TransformerSequential(args)\nlogits = model(xb)\nlogits.shape\n\ntorch.Size([16, 218, 50257])\n\n\n\nclass TransformerLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels):\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)                    \n                             \n        loss = self.loss_fct(logits, labels)\n\n        return loss\n\n\nt_loss = TransformerLoss()\nloss = t_loss(logits, yb)\nloss\n\ntensor(10.7649, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\ndata = tokenized_dataset['input_ids']\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(sequence_len, vocab_size)\nxb, yb = get_batch(data, args.batch_size)\n\nmodel = TransformerSequential(args)\nlogits = model(xb)\n\nt_loss = TransformerLoss()\nloss = t_loss(logits, yb)\nloss\n\ntensor(11.2564, grad_fn=&lt;NllLossBackward0&gt;)"
  },
  {
    "objectID": "baseclass.html",
    "href": "baseclass.html",
    "title": "Basic Class",
    "section": "",
    "text": "Trong phần này, chúng ta sẽ không giới thiệu bất kỳ kiến thức mới nào. Thay vào đó, chúng ta sẽ tái cấu trúc lại mã code từ chương trước bằng cách sử dụng các class để tạo cấu trúc code dễ đọc hơn và giống với LLAMA2 hơn.\nLý do tôi chia chương này thành một phần riêng là:\nTrong chương này, tôi sẽ không giải thích nhiều vì như tôi đã nói, chúng ta không đưa thêm kiến thức mới nào vào đây. Bạn chỉ cần sao chép mã code ở đây và thử thực hiện, bạn sẽ thấy nó tương tự với các đoạn code ở chương trước.\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nfrom einops import rearrange # einstein operation\nsample = 20\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\n\nsubset_dataset = dataset['train'][:sample]['text']\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\n\ndata = tokenized_dataset['input_ids']\ndata.shape\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\ntorch.Size([20, 219])\nclass ModelArgs:\n    def __init__(self, sequence_len, vocab_size):\n        self.batch_size = 16\n        self.n_head = 4\n        self.n_embd = 36\n        self.sequence_len = sequence_len\n        self.vocab_size = vocab_size\n\n\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(sequence_len, vocab_size)\ndef get_batch(data, batch_size):\n    idx = torch.randint(0, len(data), size=(batch_size,))\n    batch = data[idx]\n\n    xb = batch[:, :-1].contiguous()\n    yb = batch[:, 1:].contiguous()\n    \n    return xb, yb\n\nxb, yb = get_batch(data, args.batch_size)\nxb.shape, yb.shape\n\n(torch.Size([16, 218]), torch.Size([16, 218]))"
  },
  {
    "objectID": "baseclass.html#embedding",
    "href": "baseclass.html#embedding",
    "title": "Basic Class",
    "section": "Embedding",
    "text": "Embedding\n\n\nclass Embedding(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        self.sequence_len = args.sequence_len\n        \n        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n        self.position = nn.Embedding(args.sequence_len, args.n_embd)\n        \n    def forward(self, input_ids):\n        token_embd = self.wte(input_ids)\n        position_embd = self.position(torch.arange(self.sequence_len))\n        \n        input_ids_embd = token_embd + position_embd        \n        \n        return input_ids_embd\n\n\nembd = Embedding(args)\nx_embd = embd(xb)\nx_embd.shape\n\ntorch.Size([16, 218, 36])"
  },
  {
    "objectID": "baseclass.html#self-attention",
    "href": "baseclass.html#self-attention",
    "title": "Basic Class",
    "section": "Self Attention",
    "text": "Self Attention\n\n\nclass Attention(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        self.head_dim = args.n_embd // args.n_head\n        opt_size = args.n_head * self.head_dim\n        hidden_size = args.n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n    def forward(self, input_ids_embd_norm):\n        seq_len = input_ids_embd_norm.shape[1]\n        \n        qkv = self.Wqkv(input_ids_embd_norm)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n        q, k, v = qkv.unbind(2)\n        \n        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n        \n        mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n        scores += mask\n        \n        attention_weights = torch.softmax(scores, dim=-1)\n        \n        output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention_weights, v)\n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n\n\n# Normalize\nattn_norm = nn.LayerNorm(args.n_embd)\nx_embd_norm = attn_norm(x_embd)\n\nattn = Attention(args)\nattn_out = attn(x_embd_norm)\n# add residual\nattn_out += x_embd\nattn_out.shape\n\ntorch.Size([16, 218, 36])"
  },
  {
    "objectID": "baseclass.html#feed-forward",
    "href": "baseclass.html#feed-forward",
    "title": "Basic Class",
    "section": "Feed Forward",
    "text": "Feed Forward\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        hidden_size = 4 * args.n_embd\n        \n        self.fc1 = nn.Linear(args.n_embd, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, args.n_embd)\n        self.act = nn.ReLU()\n        \n    def forward(self, attn_out_norm):\n        hidden_states = self.fc1(attn_out_norm)\n        hidden_states = self.act(hidden_states)\n        ffwd_out = self.fc2(hidden_states)\n        \n        return ffwd_out\n\n\n# Normalize\nffwd_norm = nn.LayerNorm(args.n_embd)\nattn_out_norm = ffwd_norm(attn_out)\n\nffwd = FeedForward(args)\nffwd_out = ffwd(attn_out_norm)\n# add residual\nffwd_out += attn_out\nffwd_out.shape\n\ntorch.Size([16, 218, 36])"
  },
  {
    "objectID": "baseclass.html#transfomer-block",
    "href": "baseclass.html#transfomer-block",
    "title": "Basic Class",
    "section": "Transfomer Block",
    "text": "Transfomer Block\n\n\nclass TransfomerBlock(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.attention_norm = nn.LayerNorm(args.n_embd)\n        self.ffwd_norm = nn.LayerNorm(args.n_embd)\n        \n        self.attn = Attention(args)\n        self.ffwd = FeedForward(args)\n        \n    def forward(self, input_ids_embd):\n        \n        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n        \n        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n        \n        return ffwd_out\n\n\nt_block = TransfomerBlock(args)\nffwd_out = t_block(x_embd)\nffwd_out.shape\n\ntorch.Size([16, 218, 36])"
  },
  {
    "objectID": "baseclass.html#transformer",
    "href": "baseclass.html#transformer",
    "title": "Basic Class",
    "section": "Transformer",
    "text": "Transformer\n\n\nclass TransformerHead(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.norm = nn.LayerNorm(args.n_embd)\n        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n        \n    def forward(self, ffwd_out):\n        ffwd_out_norm = self.norm(ffwd_out)\n        logits = self.linear(ffwd_out_norm)\n        \n        return logits\n    \nt_head = TransformerHead(args)\nlogits = t_head(ffwd_out)\nlogits.shape\n\ntorch.Size([16, 218, 50257])\n\n\n\nclass TransformerSequential(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        n_layer = 2\n        \n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(n_layer)]\n        modules.append(TransformerHead(args))\n        \n        self.layers = nn.Sequential(*modules)\n        \n    def forward(self, input_ids):\n        logits = self.layers(input_ids)\n        \n        return logits\n\n\nmodel = TransformerSequential(args)\nlogits = model(xb)\nlogits.shape\n\ntorch.Size([16, 218, 50257])"
  },
  {
    "objectID": "baseclass.html#loss",
    "href": "baseclass.html#loss",
    "title": "Basic Class",
    "section": "Loss",
    "text": "Loss\n\nclass TransformerLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels):\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)                    \n                             \n        loss = self.loss_fct(logits, labels)\n\n        return loss\n\n\nt_loss = TransformerLoss()\nloss = t_loss(logits, yb)\nloss\n\ntensor(11.0337, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\ndata = tokenized_dataset['input_ids']\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(sequence_len, vocab_size)\nxb, yb = get_batch(data, args.batch_size)\n\nmodel = TransformerSequential(args)\nlogits = model(xb)\n\nt_loss = TransformerLoss()\nloss = t_loss(logits, yb)\nloss\n\ntensor(11.1727, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nChúng ta đã hoàn thành được cái cơ bản, ‘backbone’ của kiến trúc transformer trong LLAMA2 rồi. Bây giờ hãy chuyển sang chương tiếp theo và khám phá sâu hơn về kiến trúc thực sự của LLAMA2 nhé."
  },
  {
    "objectID": "backbone.html",
    "href": "backbone.html",
    "title": "Backbone",
    "section": "",
    "text": "Trong chươngđầu của dự án này, tôi sẽ hướng dẫn bạn qua quá trình forward propagation và tính toán giá trị loss. Mục tiêu của chúng ta là hiểu sâu hơn về phần cốt lõi, yếu tố chính (“backbone”) của kiến trúc Transformers trong LLAMA2.\nVề cơ bản, kiến trúc cốt lõi của LLAMA2 rất giống với kiến trúc Transformers (như hình ở trên).\nTrong quá trình viết code, tôi sẽ tiếp tục trình bày các ý tưởng chính của từng phần trong kiến trúc này. Tuy nhiên, nếu bạn muốn có cái nhìn chi tiết và sâu sắc hơn, bạn có thể xem phần Appendix, trong đó tôi sẽ giải thích từng phần trong kiến trúc này với chi tiết ở mức độ Character Level để bạn có thể hiểu rõ hơn về nó."
  },
  {
    "objectID": "backbone.html#setup-dataset",
    "href": "backbone.html#setup-dataset",
    "title": "Backbone",
    "section": "Setup Dataset",
    "text": "Setup Dataset\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ndataset\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 2119719\n    })\n    validation: Dataset({\n        features: ['text'],\n        num_rows: 21990\n    })\n})\n\n\nTrong dự án này, tôi sử dụng bộ dữ liệu “Tiny Datasets”, một tập dữ liệu chứa các câu tiếng Anh đơn giản, thích hợp cho trẻ 3-4 tuổi có khả năng đọc dễ dàng. Bộ dữ liệu này bao gồm hơn 2 triệu câu cho phần huấn luyện (trainset) và gần 22.000 câu cho phần thử nghiệm (valid set).\n\nsample = 20\nsubset_dataset = dataset['train'][:sample]['text']\n\nfor example in subset_dataset[:1]:\n    print(example)\n\nOne day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n\nTogether, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n\n\nỞ đây, tôi sẽ chỉ sử dụng một lượng nhỏ dữ liệu, để bắt đầu xây dựng model của mình. Mục tiêu ban đầu là tạo ra một model hoàn chỉnh. Sau khi chúng ta đạt được một model đáng tin cậy, chúng ta có thể bắt đầu sử dụng toàn bộ hoặc một lượng lớn hơn của dữ liệu để train model.\n\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token  \n\n# Tokenize the text data in the new subset dataset with padding and truncation\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\ntokenized_dataset['input_ids'][:1]\n\ntensor([[ 3198,  1110,    11,   257,  1310,  2576,  3706, 20037,  1043,   257,\n         17598,   287,   607,  2119,    13,  1375,  2993,   340,   373,  2408,\n           284,   711,   351,   340,   780,   340,   373,  7786,    13, 20037,\n          2227,   284,  2648,   262, 17598,   351,   607,  1995,    11,   523,\n           673,   714, 34249,   257,  4936,   319,   607, 10147,    13,   198,\n           198,    43,   813,  1816,   284,   607,  1995,   290,   531,    11,\n           366, 29252,    11,   314,  1043,   428, 17598,    13,  1680,   345,\n          2648,   340,   351,   502,   290, 34249,   616, 10147,  1701,  2332,\n          1995, 13541,   290,   531,    11,   366,  5297,    11, 20037,    11,\n           356,   460,  2648,   262, 17598,   290,  4259,   534, 10147,   526,\n           198,   198, 41631,    11,   484,  4888,   262, 17598,   290,   384,\n         19103,   262,  4936,   319, 20037,   338, 10147,    13,   632,   373,\n           407,  2408,   329,   606,   780,   484,   547,  7373,   290,  5742,\n          1123,   584,    13,  2293,   484,  5201,    11, 20037, 26280,   607,\n          1995,   329,  7373,   262, 17598,   290, 18682,   607, 10147,    13,\n          1119,  1111,  2936,  3772,   780,   484,   550,  4888,   290,  3111,\n          1978,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n\n\nVì dữ liệu hiện tại của chúng ta là văn bản, chúng ta cần sử dụng một tokenizer để chuyển đổi dữ liệu thành định dạng số để mô hình có thể học được.\nTôi đã quyết định sử dụng tokenizer từ mô hình EleutherAI/gpt-neo-125M vì tôi thấy nó phù hợp và tiện lợi cho giai đoạn bắt đầu. Sử dụng tokenizer từ LLAMA2 có thể đòi hỏi đăng nhập vào Hugging Face để truy cập, nhưng tôi cho rằng, ít nhất ở giai đoạn đầu, chúng ta nên giữ mọi thứ đơn giản. Trong các chương tiếp theo, tôi có thể xem xét cải tiến bằng cách sử dụng tokenizer từ LLAMA2 hoặc thậm chí tạo tokenizer từ đầu nếu cần.\n\ndata = tokenized_dataset['input_ids']\nx = data[:, :-1].contiguous()\ny = data[:, 1:].contiguous()\nx.shape, y.shape\n\n(torch.Size([20, 218]), torch.Size([20, 218]))\n\n\nTương tự như nhiều dự án khác, chúng ta cần hai thành phần chính: một là đầu vào (input) và hai là nhãn (label). Trong dự án này, đầu vào (input) sẽ bao gồm một chuỗi các từ được cung cấp, và nhãn (label) là từ tiếp theo sẽ xuất hiện trong chuỗi đó.\n\nfor i in range(5):\n    print(f\"Input: {x[0, :i+1]} --&gt; Labels: {y[0,i]}\")\n\nInput: tensor([3198]) --&gt; Labels: 1110\nInput: tensor([3198, 1110]) --&gt; Labels: 11\nInput: tensor([3198, 1110,   11]) --&gt; Labels: 257\nInput: tensor([3198, 1110,   11,  257]) --&gt; Labels: 1310\nInput: tensor([3198, 1110,   11,  257, 1310]) --&gt; Labels: 2576\n\n\n\nvocab_size = tokenizer.vocab_size\nsequence_len = x.size(1)\n\nprint(f\"Vocab Size:         {vocab_size}\")\nprint(f\"Max Sequence Length: {sequence_len}\")\n\nVocab Size:         50257\nMax Sequence Length: 218\n\n\nBởi vì chúng ta đang sử dụng EleutherAI/gpt-neo-125M làm tokenizer, Vocab Size của chúng ta sẽ là 50257. Đồng thời, độ dài tối đa của các chuỗi (max sequence length) mà chúng ta đang xử lý sẽ tạm thời là độ dài của chuỗi “x” trong chiều thứ hai, tức là “x.size(1)”."
  },
  {
    "objectID": "backbone.html#embedding",
    "href": "backbone.html#embedding",
    "title": "Backbone",
    "section": "Embedding",
    "text": "Embedding\n\n\nimport torch.nn as nn\nimport torch\n\n# Output Embedding\nn_embd = 36\nwte = nn.Embedding(vocab_size, n_embd) # word to embedding\n\ntoken_embd = wte(x)\ntoken_embd.shape\n\ntorch.Size([20, 218, 36])\n\n\n\n# Positional Encoding\nposition = nn.Embedding(sequence_len, n_embd)\n\nposition_embd = position(torch.arange(sequence_len))\nposition_embd.shape\n\ntorch.Size([218, 36])\n\n\n\nx_embd = token_embd + position_embd\nx_embd.shape\n\ntorch.Size([20, 218, 36])\n\n\nÝ tưởng cốt lõi của chúng ta là sử dụng một vectơ đặc trưng để biểu diễn mỗi từ trong bộ từ vựng (vocab size) của chúng ta. Đồng thời, chúng ta cũng áp dụng nguyên tắc này cho việc biểu diễn từng vị trí của các từ trong câu.\nHãy xem xét từ “bàn” trong vocab size của chúng ta. Khi chúng ta thực hiện quá trình embedding, từ “bàn” này được chuyển đổi thành một vectơ đặc trưng duy nhất. Điều thú vị là vectơ này có khả năng biểu diễn từ “bàn” trong nhiều ngữ cảnh khác nhau - có thể là một cái bàn gỗ, một cuộc họp trên bàn, một cái bàn đạp xe, hoặc thậm chí một phần của chân. Điều này giúp model của chúng ta hiểu và biểu diễn nhiều sắc thái và ngữ nghĩa của từ “bàn” trong các tình huống khác nhau.\nNgoài ra, chúng ta cũng tạo ra các biểu diễn độc lập cho từng vị trí của các từ trong câu. Ví dụ, từ “bàn” trong câu “Tôi vừa mua một cái bàn làm việc” sẽ có một biểu diễn vector vị trí riêng biệt, khác với từ “bàn” trong câu “Cái bàn kia thật đẹp”. Mặc dù cả hai trường hợp này có cùng nghĩa cho từ “bàn,” nhưng do vị trí của nó trong câu khác nhau (vị trí thứ 6 vs thứ 2), nó sẽ được biểu diễn theo cách khác nhau.\nTóm lại, chúng ta sử dụng vectơ đặc trưng để biểu diễn từ (word embedding) và vị trí của từ trong câu (position embedding), cho phép model hiểu và biểu diễn ngữ nghĩa và ngữ cảnh của các từ một cách hiệu quả trong các ngữ cảnh khác nhau."
  },
  {
    "objectID": "backbone.html#self-attention",
    "href": "backbone.html#self-attention",
    "title": "Backbone",
    "section": "Self Attention",
    "text": "Self Attention\n\n\n# Norm before calculate attention\nnorm = nn.LayerNorm(n_embd)\nx_embd_norm = norm(x_embd)\n\nTrong kiến trúc Transformer, có một sự thay đổi quan trọng mà chúng ta đang áp dụng so với cách truyền thống. Thay vì tính toán output của attention trước rồi mới normalize output đó, chúng ta sẽ tiến hành ngược lại: chúng ta sẽ thực hiện normalization trước, sau đó sử dụng kết quả này như input để tính attention.\nSự thay đổi này không chỉ áp dụng trong kiến trúc LLAMA2 mà còn xuất hiện rộng rãi trong hầu hết các kiến trúc Transformers hiện đại.\n\n# Multi-head Attention\nn_head = 4\n\nhead_size = n_embd // n_head\nopt_size = n_head * head_size # output size\nhead_size, opt_size\n\n(9, 36)\n\n\n\nWqkv = nn.Linear(n_embd, 3 * opt_size)\nqkv = Wqkv(x_embd_norm)\n\nfrom einops import rearrange\nqkv = rearrange(qkv, \"... (three h d) -&gt; ... three h d\", three=3, h = n_head)\n\nq, k, v = qkv.unbind(dim=2)\nq.shape, k.shape, v.shape\n\n(torch.Size([20, 218, 4, 9]),\n torch.Size([20, 218, 4, 9]),\n torch.Size([20, 218, 4, 9]))\n\n\nMục tiêu của đoạn code trên là đi tính query, key và value, còn chúng có ý nghĩa là gì thì hãy cũng xem xét ví dụ dưới đây.\nHãy tưởng tượng rằng bạn là một nhà báo nổi tiếng đang thực hiện một cuộc phỏng vấn với một ngôi sao nổi tiếng, và bạn muốn thu thập thông tin quan trọng từ cuộc trò chuyện đó.\n\nKey có thể coi như danh sách câu hỏi bạn chuẩn bị trước cuộc phỏng vấn. Mỗi câu hỏi là một Key, và mỗi câu hỏi sẽ tập trung vào một khía cạnh cụ thể của cuộc trò chuyện. Ví dụ, một Key có thể là “Bạn đã từng giành giải Oscar chưa?”\nValue là câu trả lời mà ngôi sao đưa ra cho từng câu hỏi. Mỗi câu trả lời chứa thông tin quan trọng về cuộc trò chuyện, và nó sẽ được lưu trữ và sử dụng sau này khi bạn cần nắm bắt thông tin cụ thể từ cuộc phỏng vấn. Chúng ta có thể coi câu trả lời này là “value” của câu hỏi.\nQuery là cách bạn đặt câu hỏi hoặc tìm kiếm thông tin trong cuộc phỏng vấn. Khi bạn muốn biết điều gì đó cụ thể hoặc muốn nắm bắt một thông tin quan trọng từ cuộc trò chuyện, bạn sẽ đặt câu hỏi hoặc tạo một “Query” riêng. Ví dụ, “Giới thiệu về những vai diễn nổi bật nhất của bạn?” có thể là một Query.\n\nKhi bạn đặt một câu hỏi (Query), model sẽ so sánh nó với danh sách các câu hỏi trước đó (Key) và quyết định câu trả lời nào (Value) chứa thông tin phù hợp nhất với câu hỏi của bạn. Điều này giống như việc bạn tập trung vào câu hỏi cụ thể nào trong cuộc trò chuyện để thu thập thông tin bạn cần.\n\n# Masked multi-head\nimport math\nsoftmax_scale = 1.0 / math.sqrt(q.shape[-1])\n\nscores = torch.einsum(\"bthd,bshd-&gt;bhts\", q, k * softmax_scale)\n\n# Masking\nmask = torch.triu(torch.full((sequence_len, sequence_len), -10000.0), 1)\nscores = scores + mask\n\nattention = torch.softmax(scores, dim=-1)\n\nÝ tưởng chính ở đây là ta đang xây dựng một hệ thống dự đoán từ tiếp theo trong câu văn dựa trên các từ đã xuất hiện trước đó. Để thực hiện điều này, mỗi từ cần được dự đoán sẽ đóng vai trò là một Query, và các từ đã xuất hiện trước đó sẽ đóng vai trò là Key. Chúng ta sau đó so sánh tỉ lệ phù hợp giữa các query và các key để xác định những từ nào quan trọng hơn và sẽ được sử dụng để dự đoán từ tiếp theo.\nĐặc điểm quan trọng là chúng ta sử dụng một cơ chế “masking” để che đi thông tin của các từ đứng sau từ cần dự đoán. Điều này giúp mô hình tập trung vào việc sử dụng thông tin từ các từ trước đó để dự đoán từ tiếp theo mà không bị ảnh hưởng bởi các từ sau đó trong câu.\n\nattn_out = torch.einsum(\"bhts,bshd-&gt;bthd\", attention, v)\n\nattn_out = rearrange(attn_out, \"... h d -&gt; ... (h d)\")\n\nout_proj = nn.Linear(opt_size, n_embd)\nattn_out = out_proj(attn_out)\n\n# Add residual\nresidual = x_embd\n\nattn_out += residual\nattn_out.shape\n\ntorch.Size([20, 218, 36])\n\n\nQuá trình này được gọi là Self-Attention, vì điểm đặc biệt là giá trị của key và value được tạo ra từ chính bản thân câu văn hoặc chuỗi đầu vào.\nSelf-Attention là một khía cạnh quan trọng trong kiến trúc Transformer, vì nó cho phép mô hình xác định mức độ quan trọng của các từ hoặc phần tử trong câu văn đối với từ hoặc phần tử cụ thể khác trong câu văn đó. Điều này giúp mô hình xử lý ngôn ngữ tự nhiên một cách linh hoạt và hiểu ngữ cảnh một cách tốt hơn."
  },
  {
    "objectID": "backbone.html#cross-attention",
    "href": "backbone.html#cross-attention",
    "title": "Backbone",
    "section": "Cross Attention",
    "text": "Cross Attention\n\nKhác với Self Attention, Cross Attention đặt ra sự khác biệt bằng cách mà giá trị key và value không đến từ bản thân câu văn, mà chúng đến từ nguồn thông tin bên ngoài.\nHãy cùng tưởng tượng một ví dụ để hiểu rõ hơn: Self Attention có thể được tưởng tượng như bạn đặt ra những câu hỏi cho chính bản thân mình (Key) và tự mình trả lời chúng (Value). Trong một ngày khác, bạn tiếp tục đặt ra những câu hỏi mới, nhưng lần này bạn không tự trả lời mà bạn xem xét những câu hỏi bạn đã đặt trước đó (Query) và xem câu hỏi nào (Key) phù hợp nhất với câu hỏi hiện tại (Query), sau đó bạn sẽ dựa vào câu trả lời đó (Value).\nCross Attention, ngược lại, có thể được tưởng tượng như bạn tham gia vào một cuộc phỏng vấn với một diễn viên nổi tiếng. Trong cuộc phỏng vấn, bạn đặt ra những câu hỏi (Key) và ghi chép lại câu trả lời của người diễn viên đó (Value). Sau đó, vào một thời điểm khác, bạn tự đặt ra những câu hỏi cho chính mình (Query) và xem xét xem câu hỏi nào bạn đã đặt trong cuộc phỏng vấn (Key) phù hợp nhất với câu hỏi của bạn (Query) và sử dụng câu trả lời từ cuộc phỏng vấn đó (Value).\nTuy nhiên trong dự án hiện tại của chúng ta, không cần thiết phải sử dụng Cross Attention vì chúng ta không cần sử dụng các key và value từ nguồn bên ngoài. Do đó, chúng ta có thể bỏ qua bước Cross Attention và tiếp tục với quá trình Feed Forward."
  },
  {
    "objectID": "backbone.html#feed-forward",
    "href": "backbone.html#feed-forward",
    "title": "Backbone",
    "section": "Feed Forward",
    "text": "Feed Forward\n\n\n# Normalize before calc feed forward\nnorm = nn.LayerNorm(n_embd)\nattn_out_norm = norm(attn_out)\n\nTương tự như quá trình Attention, chúng ta sử dụng normalize đầu ra của Attention trước khi đưa nó vào lớp Feed Forward.\n\nhidden_size = 4 * n_embd\n\nlinear_1 = nn.Linear(n_embd, hidden_size)\nlinear_2 = nn.Linear(hidden_size, n_embd)\n\nact = nn.ReLU()\n\n# Feed forward output\nhidden_states = linear_1(attn_out_norm)\nhidden_states = act(hidden_states)\nffwd_out = linear_2(hidden_states)\n\n# Add residual\nresidual = attn_out\nffwd_out += residual\nffwd_out.shape\n\ntorch.Size([20, 218, 36])\n\n\nVề bản chất, phần Feed Forward trong kiến trúc Transformer không quá phức tạp. Nó bao gồm một hidden layer và một output layer, kèm theo một hàm activation. Mặc dù có một số cải tiến trong LLAMA2, nhưng nguyên tắc cơ bản vẫn thì giống như đã được mô tả ở trên."
  },
  {
    "objectID": "backbone.html#transformer-block",
    "href": "backbone.html#transformer-block",
    "title": "Backbone",
    "section": "Transformer Block",
    "text": "Transformer Block\n\n\n# Self Attention\n# Normalize\nattention_norm = nn.LayerNorm(n_embd)\nx_embd_norm = attention_norm(x_embd)\n# Multi-head Attention\nn_head = 4\nhead_size = n_embd // n_head\nopt_size = n_head * head_size # output size\n\nWqkv = nn.Linear(n_embd, 3 * opt_size)\nqkv = Wqkv(x_embd_norm)\nqkv = rearrange(qkv, \"... (three h d) -&gt; ... three h d\", three=3, h = n_head)\nq, k, v = qkv.unbind(dim=2)\n# Masked multi-head\nsoftmax_scale = 1.0 / math.sqrt(q.shape[-1])\nscores = torch.einsum(\"bthd,bshd-&gt;bhts\", q, k * softmax_scale)\nmask = torch.triu(torch.full((sequence_len, sequence_len), -10000.0), 1)\nscores = scores + mask\n\nattention = torch.softmax(scores, dim=-1)\n\nattn_out = torch.einsum(\"bhts,bshd-&gt;bthd\", attention, v)\nattn_out = rearrange(attn_out, \"... h d -&gt; ... (h d)\")\n# Attention output\nout_proj = nn.Linear(opt_size, n_embd)\nattn_out = out_proj(attn_out)\n# Add residual\nresidual = x_embd\nattn_out += residual\n\n\n# Feed Forward\nhidden_size = 4 * n_embd\nlinear_1 = nn.Linear(n_embd, hidden_size)\nlinear_2 = nn.Linear(hidden_size, n_embd)\nact = nn.ReLU()\n\n# Normalize\nffwd_norm = nn.LayerNorm(n_embd)\nattn_out_norm = ffwd_norm(attn_out)\n# Feed forward output\nhidden_states = linear_1(attn_out_norm)\nhidden_states = act(hidden_states)\nffwd_out = linear_2(hidden_states)\n# Add residual\nresidual = attn_out\nffwd_out += residual\nffwd_out.shape\n\ntorch.Size([20, 218, 36])\n\n\nQuá trình từ input đã được embedding đi qua lớp Attention và lớp Feed Forward mà chúng ta vừa hoàn thành được gọi là một “block” trong kiến trúc Transformers. Trong hình ảnh, việc có một khung bao quanh quá trình Attention và Feed Forward ngụ ý rằng chúng hoạt động cùng nhau như một block duy nhất. Chữ “Nx” chỉ ra rằng ta có thể có nhiều block tùy ý. Trong trường hợp này, tôi chỉ sử dụng 1 block làm ví dụ, và nếu bạn muốn sử dụng 2 block, bạn có thể đơn giản sao chép toàn bộ đoạn code ở trên và dán vào một ô mới để tạo ra 2 block."
  },
  {
    "objectID": "backbone.html#transformer-head",
    "href": "backbone.html#transformer-head",
    "title": "Backbone",
    "section": "Transformer Head",
    "text": "Transformer Head\n\n\n# Normalize output feed forward\nnorm = nn.LayerNorm(n_embd)\noutput = norm(ffwd_out)\n\nlast_linear = nn.Linear(n_embd, vocab_size)\nlogits = last_linear(output)\n\nlogits.shape\n\ntorch.Size([20, 218, 50257])\n\n\nBởi vì có sự thay đổi trong quá trình normalize trong Feed Forward, nơi chúng ta thực hiện việc normalize trước khi truyền vào lớp Feed Forward. Vì vậy, chúng ta cần điều chỉnh kiến trúc model bằng cách thêm lớp normalize cho output của lớp Feed Forward trước khi truyền nó vào lớp Linear cuối cùng."
  },
  {
    "objectID": "backbone.html#loss",
    "href": "backbone.html#loss",
    "title": "Backbone",
    "section": "Loss",
    "text": "Loss\n\nloss_fct = nn.CrossEntropyLoss()\n\nlogits  = logits.view(-1, logits.shape[-1])\nlabels = y.view(-1)\n\nloss = loss_fct(logits, labels)\nloss\n\ntensor(10.9201, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\nVậy là chúng ta đã hoàn thành toàn bộ kiến trúc Transformers được mô tả trong hình ảnh ở trên, và điều này cũng đồng nghĩa với việc chúng ta đã hiểu “backbone” của kiến trúc Transformers trong LLAMA2. Tuy nhiên, trong chương này, chúng ta chỉ đang viết code mà chưa sử dụng bất kỳ class nào. Trong chương tiếp theo, tôi sẽ sắp xếp code vào các class để làm cho kiến trúc trở nên có cấu trúc và giống với LLAMA2 hơn."
  },
  {
    "objectID": "llamap2.html",
    "href": "llamap2.html",
    "title": "Llama2 Architecture (P2)",
    "section": "",
    "text": "”\nTrong chương này, chúng ta sẽ cùng tìm hiểu về 3 chi tiết còn lại: RMS Norm, Group Multi Query Attention with KV cache, và Feed Forward SwiGLU. Hãy cùng khám phá những khái niệm mới này và tìm hiểu cách chúng hoạt động!\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nimport math\nfrom einops import rearrange # einstein operation\nsample = 20\n\ndataset = load_dataset(\"roneneldan/TinyStories\")\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\ntokenizer.pad_token = tokenizer.eos_token\n\nsubset_dataset = dataset['train'][:sample]['text']\ntokenized_dataset = tokenizer(\n    subset_dataset,\n    return_tensors='pt',\n    padding=True,  # Enable padding\n    truncation=True  # Enable truncation\n)\n\ndata = tokenized_dataset['input_ids']\ndata.shape\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\ntorch.Size([20, 219])\nclass ModelArgs:\n    def __init__(self, sequence_len, vocab_size):\n\n        self.rotary_dim = 3\n        \n        self.n_layer = 2\n        self.batch_size = 16\n        self.n_head = 4\n        self.n_embd = 36\n        self.sequence_len = sequence_len\n        self.vocab_size = vocab_size\n\n\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(sequence_len, vocab_size)\ndef get_batch(data, batch_size):\n    idx = torch.randint(0, len(data), size=(batch_size,))\n    batch = data[idx]\n\n    xb = batch[:, :-1].contiguous()\n    yb = batch[:, 1:].contiguous()\n    \n    return xb, yb\n\nxb, yb = get_batch(data, args.batch_size)\nxb.shape, yb.shape\n\n(torch.Size([16, 218]), torch.Size([16, 218]))"
  },
  {
    "objectID": "llamap2.html#embedding",
    "href": "llamap2.html#embedding",
    "title": "Llama2 Architecture (P2)",
    "section": "Embedding",
    "text": "Embedding\n”\n\nclass Embedding(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        self.wte = nn.Embedding(args.vocab_size, args.n_embd)\n        \n    def forward(self, input_ids):\n        input_ids_embd = self.wte(input_ids)\n        \n        return input_ids_embd\n    \nembd = Embedding(args)\nx_embd = embd(xb)\nx_embd.shape\n\ntorch.Size([16, 218, 36])"
  },
  {
    "objectID": "llamap2.html#rotary-embedding",
    "href": "llamap2.html#rotary-embedding",
    "title": "Llama2 Architecture (P2)",
    "section": "Rotary Embedding",
    "text": "Rotary Embedding\n”\n\nclass RotaryPositionEmbedding(nn.Module):\n    def __init__(self, args:ModelArgs, base = 10000):\n        super().__init__()\n        self.rotary_dim  = args.rotary_dim\n        \n        inv_freq = 1.0 / (base ** (torch.arange(0, self.rotary_dim, 2) / self.rotary_dim ))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        \n        self.cos_cache = None\n        self.sin_cache = None\n        \n    def forward(self, qkv):\n        seqlen = qkv.shape[1]\n        \n        # Update cos sin cache\n        t = torch.arange(seqlen)\n        freqs = torch.outer(t, self.inv_freq)\n        \n        self.cos_cache = torch.cos(freqs)\n        self.sin_cache = torch.sin(freqs)\n        \n        # Apply rotary qkv\n        rotary_dim = self.cos_cache.shape[1]\n        rotary_dim *= 2\n        \n        q_rot = qkv[:, :, 0, :, :rotary_dim]\n        q_pass = qkv[:, :, 0, :, rotary_dim:]\n        \n        k_rot = qkv[:, :, 1, :, :rotary_dim]\n        k_pass = qkv[:, :, 1, :, rotary_dim:]\n        \n        # Splits the queries and keys in half\n        q1, q2 = q_rot.chunk(2, dim=-1)\n        k1, k2 = k_rot.chunk(2, dim=-1)\n        c, s = rearrange(self.cos_cache, \"t d -&gt; t 1 d\"), rearrange(self.sin_cache, \"t d -&gt; t 1 d\")\n        \n        # Computes the new keys and queries\n        q_rot = torch.cat([q1 * c - q2 * s, q1 * s - q2 * c], dim=-1)\n        k_rot = torch.cat([k1 * c - k2 * s, k1 * s - k2 * c], dim = -1)\n        \n        return torch.cat(\n            [\n                torch.cat([q_rot, q_pass], dim=-1).unsqueeze(2),\n                torch.cat([k_rot, k_pass], dim=-1).unsqueeze(2),\n                qkv[:, :, 2:3, :, :]\n            ],\n            dim=2\n        )"
  },
  {
    "objectID": "llamap2.html#rms-norm",
    "href": "llamap2.html#rms-norm",
    "title": "Llama2 Architecture (P2)",
    "section": "RMS Norm",
    "text": "RMS Norm\n”\n\nclass RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float()).type_as(x)\n        return output * self.weight\n    \n    \nattn_norm = RMSNorm(args.n_embd)\nx_embd_norm = attn_norm(x_embd)\nx_embd_norm.shape\n\ntorch.Size([16, 218, 36])\n\n\nRMS Norm thực chất là một biến thể của Layer Norm. Ý tưởng cơ bản là thay vì sử dụng Layer Norm làm quá trình normalize dữ liệu, họ chuyển sang sử dụng RMS Norm. Cụ thể, trong ảnh trên thay vì áp dụng layer norm cho x_embd để tính toán qkv, họ thay thế nó bằng RMS Norm. Điều này cũng áp dụng tương tự cho việc normalize output. Sự thay đổi này có thể mang lại một số ưu điểm cụ thể trong quá trình xử lý và huấn luyện mô hình."
  },
  {
    "objectID": "llamap2.html#self-attention",
    "href": "llamap2.html#self-attention",
    "title": "Llama2 Architecture (P2)",
    "section": "Self Attention",
    "text": "Self Attention\n”\n\nimport torch.nn.functional as F\n\nclass Attention(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.rotary_emb = RotaryPositionEmbedding(args)\n        \n        self.head_dim = args.n_embd // args.n_head\n        opt_size = args.n_head * self.head_dim\n        hidden_size = args.n_embd\n        \n        self.Wqkv = nn.Linear(hidden_size, 3 * opt_size)\n        self.out_proj = nn.Linear(opt_size, hidden_size)\n        \n    def forward(self, input_ids_embd_norm):\n        seq_len = input_ids_embd_norm.shape[1]\n        \n        qkv = self.Wqkv(input_ids_embd_norm)\n        qkv = rearrange(qkv, 'b t (three h d) -&gt; b t three h d', three=3, d=self.head_dim)\n        \n        # Rotary Query & Key\n        qkv = self.rotary_emb(qkv)\n        \n        q, k, v = qkv.unbind(2)\n        \n        # New code\n        # --------------------------------------------------------------------------------\n        output = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n\n        # softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n        # scores = torch.einsum(\"bthd, bshd -&gt; bhts\", q, k * softmax_scale)\n        \n        # mask = torch.triu(torch.full((seq_len, seq_len), -10000), 1)\n        # scores += mask\n        \n        # attention_weights = torch.softmax(scores, dim=-1)\n        \n        # output = torch.einsum(\"bhts, bshd -&gt; bthd\", attention_weights, v)\n        # ----------------------------------------------------------------------------------\n        \n        output = rearrange(output, \"... h d -&gt; ... (h d)\")\n\n        attn_out = self.out_proj(output)\n        \n        return attn_out\n    \n# Normalize\nattn_norm = RMSNorm(args.n_embd)\nx_embd_norm = attn_norm(x_embd)\n\nattn = Attention(args)\nattn_out = attn(x_embd_norm)\n# add residual\nattn_out += x_embd\nattn_out.shape\n\ntorch.Size([16, 218, 36])\n\n\nTrong phương pháp trước đây, việc tính toán lại các scores một cách lặp đi lặp lại dẫn đến sự lãng phí đáng kể về hiệu suất tính toán. Ví dụ, trong câu “Tôi thích chạy bộ” khi chúng ta cố gắng dự đoán từ “thích” dựa trên từ “Tôi”, chúng ta thực hiện tính scores bằng cách nhân toàn bộ query của từ cần dự đoán với toàn bộ key của các từ khác, sau đó phải loại bỏ scores của các từ không cần thiết (trong trường hợp này là “thích chạy bộ”). Tương tự, khi dự đoán từ “chạy”, chúng ta lại thực hiện lại quá trình tính scores bằng cách nhân toàn bộ query với các key, sau đó phải loại bỏ scores của các từ không cần thiết (“chạy bộ”). Điều này dẫn đến một sự lãng phí lớn về hiệu suất tính toán.\nÝ tưởng chính của phương pháp “Group Multi-Query Attention with KV cache” là giảm thiểu sự lãng phí này bằng cách tận dụng lại các kết quả đã được tính toán và lưu trữ trước đó thay vì tính toán lại từ đầu. Hàm F.scaled_dot_product_attention thực hiện ý tưởng này và đồng thời giúp mã nguồn trở nên rõ ràng hơn đáng kể. Sử dụng cách tiếp cận này giúp chúng ta tái sử dụng các kết quả trước đó đã được tính toán và lưu trữ, từ đó giảm thiểu việc tính toán lại và giúp mã nguồn trở nên dễ đọc và dễ hiểu hơn rất nhiều."
  },
  {
    "objectID": "llamap2.html#feed-forward",
    "href": "llamap2.html#feed-forward",
    "title": "Llama2 Architecture (P2)",
    "section": "Feed Forward",
    "text": "Feed Forward\n”\n\nclass FeedForward(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        multiple_of = 5\n        \n        hidden_dim = 4 * args.n_embd\n        hidden_dim = int(2 * hidden_dim / 3)\n        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n        \n        self.w1 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, args.n_embd, bias=False)\n        \n        self.w3 = nn.Linear(args.n_embd, hidden_dim, bias=False)\n        \n        self.act = nn.SiLU()\n        \n    def forward(self, attn_out_norm):\n        \n        hidden_states = self.w1(attn_out_norm) * self.w3(attn_out_norm)\n        hidden_states = self.act(hidden_states)\n        \n        ffwd_out = self.w2(hidden_states)\n        \n        return ffwd_out\n    \n# Normalize\nffwd_norm = RMSNorm(args.n_embd)\nattn_out_norm = ffwd_norm(attn_out)\n\nffwd = FeedForward(args)\nffwd_out = ffwd(attn_out_norm)\n# add residual\nffwd_out += attn_out\nffwd_out.shape\n\ntorch.Size([16, 218, 36])\n\n\nGiống như trước đó, chúng ta sẽ sử dụng RMS Norm để normalize output attention thay vì Layer Norm.\nFeed Forward SwiGLU là một cải tiến của phương pháp Feed Forward thông thường, nhằm tăng cường khả năng học và biểu diễn của mô hình. Bằng cách tăng cường phức tạp hóa cấu trúc của lớp feed forward, SwiGLU có thể học được các mối quan hệ phức tạp và đặc trưng của dữ liệu một cách hiệu quả hơn. Việc tăng cường tính phức tạp của kiến trúc này thường đi kèm với việc sử dụng các phép tính toán và hàm activation phức tạp hơn (SiLU), nhằm tăng tính linh hoạt và mạnh mẽ của mô hình trong việc xử lý dữ liệu phức tạp và đa dạng."
  },
  {
    "objectID": "llamap2.html#transformer-block",
    "href": "llamap2.html#transformer-block",
    "title": "Llama2 Architecture (P2)",
    "section": "Transformer Block",
    "text": "Transformer Block\n”\n\nclass TransfomerBlock(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.attention_norm = RMSNorm(args.n_embd)\n        self.ffwd_norm = RMSNorm(args.n_embd)\n        \n        self.attn = Attention(args)\n        self.ffwd = FeedForward(args)\n        \n    def forward(self, input_ids_embd):\n        \n        attn_out = input_ids_embd + self.attn(self.attention_norm(input_ids_embd))\n        \n        ffwd_out = attn_out + self.ffwd(self.ffwd_norm(attn_out))\n        \n        return ffwd_out\n    \nt_block = TransfomerBlock(args)\nffwd_out = t_block(x_embd)\nffwd_out.shape\n\ntorch.Size([16, 218, 36])\n\n\nLLAMA2 architecture có những cải tiến hơn so với transformer architecture mà chúng ta đã học trước đó. Các cải tiến này chủ yếu tập trung vào các class Attention và Feed Forward. Do đó, các class còn lại cơ bản chỉ thay đổi việc sử dụng normalize từ Layer Norm sang RMS Norm, các code khác đều sẽ được giữ nguyên."
  },
  {
    "objectID": "llamap2.html#transformer",
    "href": "llamap2.html#transformer",
    "title": "Llama2 Architecture (P2)",
    "section": "Transformer",
    "text": "Transformer\n\n\nclass TransformerHead(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        self.norm = RMSNorm(args.n_embd)\n        self.linear = nn.Linear(args.n_embd, args.vocab_size)\n        \n    def forward(self, ffwd_out):\n        ffwd_out_norm = self.norm(ffwd_out)\n        logits = self.linear(ffwd_out_norm)\n        \n        return logits\n    \nt_head = TransformerHead(args)\nlogits = t_head(ffwd_out)\nlogits.shape\n\ntorch.Size([16, 218, 50257])\n\n\n\nclass TransformerSequential(nn.Module):\n    def __init__(self, args:ModelArgs):\n        super().__init__()\n        \n        modules = [Embedding(args)]\n        modules += [TransfomerBlock(args) for _ in range(args.n_layer)]\n        modules.append(TransformerHead(args))\n        \n        self.layers = nn.Sequential(*modules)\n        \n    def forward(self, input_ids):\n        return self.layers(input_ids)\n    \nmodel = TransformerSequential(args)\nlogits = model(xb)\nlogits.shape\n\ntorch.Size([16, 218, 50257])"
  },
  {
    "objectID": "llamap2.html#loss",
    "href": "llamap2.html#loss",
    "title": "Llama2 Architecture (P2)",
    "section": "Loss",
    "text": "Loss\n\nclass TransformerLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fct = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, labels):\n        logits = logits.view(-1, logits.shape[-1])\n        labels = labels.view(-1)                    \n                             \n        loss = self.loss_fct(logits, labels)\n\n        return loss\n    \nt_loss = TransformerLoss()\nloss = t_loss(logits, yb)\nloss\n\ntensor(10.8817, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\ndata = tokenized_dataset['input_ids']\nsequence_len = data.size(1) - 1\nvocab_size = tokenizer.vocab_size\n\nargs = ModelArgs(sequence_len, vocab_size)\nxb, yb = get_batch(data, args.batch_size)\n\nmodel = TransformerSequential(args)\nlogits = model(xb)\n\nt_loss = TransformerLoss()\nloss = t_loss(logits, yb)\nloss\n\ntensor(11.0817, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nVậy là chúng ta đã hoàn thành kiến trúc transformer của LLAMA2.Trong chương tiếp theo, chúng ta sẽ đào sâu vào việc khởi tạo trọng số (weight initialization). Có vẻ như phương pháp khởi tạo trọng số mặc định của PyTorch không còn phù hợp nữa, và LLAMA2 đã sử dụng một phương pháp khởi tạo trọng số khác. Hãy cùng tìm hiểu về điều này trong chương tiếp theo."
  }
]